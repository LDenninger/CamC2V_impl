import logging
import pdb

import torch
import torch.nn.functional as F
from einops import rearrange, repeat
from functools import partial

from lvdm.models.utils_diffusion import timestep_embedding
from lvdm.modules.attention import SpatialTransformer, TemporalTransformer
from lvdm.modules.networks.openaimodel3d import TimestepBlock
from lvdm.common import (
    checkpoint,
    exists,
    default,
)
import math
try:
    import xformers
    import xformers.ops

    XFORMERS_IS_AVAILBLE = True
except:
    XFORMERS_IS_AVAILBLE = False

mainlogger = logging.getLogger('mainlogger')


# add RT input to forward of unet
def new_forward_for_unet(self, x, timesteps, context=None, features_adapter=None, fs=None, camera_condition=None,
                         additional_condition=None, **kwargs):
    """
        camera_condition (dict): ["pluker_embedding_feautres", "sample_locs_dict", "cond_frame_index", "add_type"]
    """
    #import ipdb; ipdb.set_trace()
    b, _, t, _, _ = x.shape
    t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False).type(x.dtype)
    emb = self.time_embed(t_emb)

    ## repeat t times for context [(b t) 77 768] & time embedding
    ## check if we use per-frame image conditioning
    _, l_context, _ = context.shape
    if l_context == 77 + t * 16:  ## !!! HARD CODE here                     # interp_mode
        context_text, context_img = context[:, :77, :], context[:, 77:, :]
        context_text = context_text.repeat_interleave(repeats=t, dim=0)
        context_img = rearrange(context_img, 'b (t l) c -> (b t) l c', t=t)
        context = torch.cat([context_text, context_img], dim=1)
    else:
        context = context.repeat_interleave(repeats=t, dim=0)           # single cond frame
    emb = emb.repeat_interleave(repeats=t, dim=0)

    ## always in shape (b t) c h w, except for temporal layer
    x = rearrange(x, 'b c t h w -> (b t) c h w')

    ## combine emb
    if self.fs_condition:
        if fs is None:
            fs = torch.tensor(
                [self.default_fs] * b, dtype=torch.long, device=x.device)
        fs_emb = timestep_embedding(fs, self.model_channels, repeat_only=False).type(x.dtype)

        fs_embed = self.fps_embedding(fs_emb)
        fs_embed = fs_embed.repeat_interleave(repeats=t, dim=0)
        emb = emb + fs_embed

    h = x.type(self.dtype)
    adapter_idx = 0
    hs = []
    for id, module in enumerate(self.input_blocks):
        ########################################### only change here, add camera_condition input ###########################################
        if camera_condition is not None:
            camera_condition_input = {}
            camera_condition_input['h'] = h.shape[-2]
            camera_condition_input['w'] = h.shape[-1]
            for key, value in camera_condition.items():
                if 'pluker' in key and value is not None:
                    if self.input_ds[id] in self.attention_resolutions:
                        feature_id = int(math.log2(self.input_ds[id]))
                        camera_condition_input[key] = value[feature_id]
                else:
                    camera_condition_input[key] = value
        else:
            camera_condition_input = None
        h = module(h, emb, context=context, batch_size=b, camera_condition=camera_condition_input, additional_condition=additional_condition)
        if id == 0 and self.addition_attention:
            h = self.init_attn(h, emb, context=context, batch_size=b, camera_condition=None)
        ########################################### only change here, add camera_condition input ###########################################
        ## plug-in adapter features
        if ((id + 1) % 3 == 0) and features_adapter is not None:
            h = h + features_adapter[adapter_idx]
            adapter_idx += 1
        hs.append(h)
    if features_adapter is not None:
        assert len(features_adapter) == adapter_idx, 'Wrong features_adapter'

    ########################################### only change here, add camera_condition input ###########################################
    if camera_condition is not None:
        camera_condition_input = {}
        camera_condition_input['h'] = h.shape[-2]
        camera_condition_input['w'] = h.shape[-1]
        for key, value in camera_condition.items():
            if 'pluker' in key and value is not None:
                camera_condition_input[key] = value[-1]
            else:
                camera_condition_input[key] = value
    else:
        camera_condition_input = None
    h = self.middle_block(h, emb, context=context, batch_size=b, camera_condition=camera_condition_input, additional_condition=additional_condition)
    ########################################### only change here, add camera_condition input ###########################################

    for id, module in enumerate(self.output_blocks):
        #import ipdb; ipdb.set_trace()
        h = torch.cat([h, hs.pop()], dim=1)
        ########################################### only change here, add camera_condition input ###########################################
        if camera_condition is not None:
            camera_condition_input = {}
            camera_condition_input['h'] = h.shape[-2]
            camera_condition_input['w'] = h.shape[-1]
            for key, value in camera_condition.items():
                if 'pluker' in key and value is not None:
                    if self.output_ds[id] in self.attention_resolutions:
                        feature_id = int(math.log2(self.output_ds[id]))
                        camera_condition_input[key] = value[feature_id]
                else:
                    camera_condition_input[key] = value
        else:
            camera_condition_input = None

        h = module(h, emb, context=context, batch_size=b, camera_condition=camera_condition_input, additional_condition=additional_condition)
        ########################################### only change here, add camera_condition input ###########################################
    h = h.type(x.dtype)
    y = self.out(h)

    # reshape back to (b c t h w)
    y = rearrange(y, '(b t) c h w -> b c t h w', b=b)
    return y

def new_efficient_forward_for_cross_attention(self, x, context=None, mask=None, image_emb_func: callable=None):
    #import ipdb; ipdb.set_trace()
    spatial_self_attn = (context is None)
    k_ip, v_ip, out_ip = None, None, None
    
    q = self.to_q(x)
    context = default(context, x)

    if self.image_cross_attention and not spatial_self_attn:
        context, context_image = context[:, :self.text_context_len, :], context[:, self.text_context_len:, :]
        k = self.to_k(context)
        v = self.to_v(context)
        ########################################### only change here, add pose guided cross attention ###########################################
        if image_emb_func is None:
            k_ip = self.to_k_ip(context_image)
            v_ip = self.to_v_ip(context_image)
        ########################################### only change here, add pose guided cross attention ###########################################
    else:
        if not spatial_self_attn:
            context = context[:, :self.text_context_len, :]
        k = self.to_k(context)
        v = self.to_v(context)

    b, _, _ = q.shape
    q, k, v = map(
        lambda t: t.unsqueeze(3)
        .reshape(b, t.shape[1], self.heads, self.dim_head)
        # .permute(0, 2, 1, 3)
        # .reshape(b * self.heads, t.shape[1], self.dim_head)
        .contiguous(),
        (q, k, v),
    )
    # actually compute the attention, what we cannot get enough of
    out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=None)

    ## for image cross-attention
    if k_ip is not None:
        k_ip, v_ip = map(
            lambda t: t.unsqueeze(3)
            .reshape(b, t.shape[1], self.heads, self.dim_head)
            # .permute(0, 2, 1, 3)
            # .reshape(b * self.heads, t.shape[1], self.dim_head)
            .contiguous(),
            (k_ip, v_ip),
        )
    
        out_ip = xformers.ops.memory_efficient_attention(q, k_ip, v_ip, attn_bias=None, op=None)
        out_ip = (
            out_ip.unsqueeze(0)
            # .reshape(b, self.heads, out.shape[1], self.dim_head)
            # .permute(0, 2, 1, 3)
            .reshape(b, out.shape[1], self.heads * self.dim_head)
        )
    ########################################### only change here, add pose guided cross attention ###########################################
    elif image_emb_func is not None:
        out_ip = image_emb_func(q, context_image)
    ########################################### only change here, add pose guided cross attention ###########################################

    if exists(mask):
        raise NotImplementedError
    out = (
        out.unsqueeze(0)
        # .reshape(b, self.heads, out.shape[1], self.dim_head)
        # .permute(0, 2, 1, 3)
        .reshape(b, out.shape[1], self.heads * self.dim_head)
    )
    if out_ip is not None:
        if self.image_cross_attention_scale_learnable:
            out = out + self.image_cross_attention_scale * out_ip * (torch.tanh(self.alpha) + 1)
        else:
            out = out + self.image_cross_attention_scale * out_ip

    return self.to_out(out)

def new_forward_for_cross_attention(self, x, context=None, mask=None, image_emb_func: callable=None):
    #import ipdb; ipdb.set_trace()

    spatial_self_attn = (context is None)
    k_ip, v_ip, out_ip = None, None, None

    h = self.heads
    q = self.to_q(x)
    context = default(context, x)

    if self.image_cross_attention and not spatial_self_attn:
        context, context_image = context[:, :self.text_context_len, :], context[:, self.text_context_len:, :]
        k = self.to_k(context)
        v = self.to_v(context)
        if image_emb_func is None:
            k_ip = self.to_k_ip(context_image)
            v_ip = self.to_v_ip(context_image)
    else:
        if not spatial_self_attn:
            context = context[:, :self.text_context_len, :]
        k = self.to_k(context)
        v = self.to_v(context)

    q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))

    sim = torch.einsum('b i d, b j d -> b i j', q, k) * self.scale
    if self.relative_position:
        len_q, len_k, len_v = q.shape[1], k.shape[1], v.shape[1]
        k2 = self.relative_position_k(len_q, len_k)
        sim2 = torch.einsum('b t d, t s d -> b t s', q, k2) * self.scale  # TODO check
        sim += sim2
    del k

    if exists(mask):
        ## feasible for causal attention mask only
        max_neg_value = -torch.finfo(sim.dtype).max
        mask = repeat(mask, 'b i j -> (b h) i j', h=h)
        sim.masked_fill_(~(mask > 0.5), max_neg_value)

    # attention, what we cannot get enough of
    sim = sim.softmax(dim=-1)

    out = torch.einsum('b i j, b j d -> b i d', sim, v)
    if self.relative_position:
        v2 = self.relative_position_v(len_q, len_v)
        out2 = torch.einsum('b t s, t s d -> b t d', sim, v2)  # TODO check
        out += out2
    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)

    ## for image cross-attention
    if k_ip is not None:
        k_ip, v_ip = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (k_ip, v_ip))
        sim_ip = torch.einsum('b i d, b j d -> b i j', q, k_ip) * self.scale
        del k_ip
        sim_ip = sim_ip.softmax(dim=-1)
        out_ip = torch.einsum('b i j, b j d -> b i d', sim_ip, v_ip)
        out_ip = rearrange(out_ip, '(b h) n d -> b n (h d)', h=h)
    ########################################### only change here, add pose guided cross attention ###########################################
    elif image_emb_func is not None:
        out_ip = image_emb_func(q, context_image)
    ########################################### only change here, add pose guided cross attention ###########################################

    if out_ip is not None:
        if self.image_cross_attention_scale_learnable:
            out = out + self.image_cross_attention_scale * out_ip * (torch.tanh(self.alpha) + 1)
        else:
            out = out + self.image_cross_attention_scale * out_ip

    return self.to_out(out)


# add camera_condition input to forward of TemporalTransformer
def new_forward_for_TimestepEmbedSequential(self, x, emb, context=None, batch_size=None, camera_condition=None, additional_condition=None,):
    for layer in self:
        if isinstance(layer, TimestepBlock):
            x = layer(x, emb, batch_size=batch_size)
        elif isinstance(layer, SpatialTransformer):
            x = layer(x, context)
        elif isinstance(layer, TemporalTransformer):
            x = rearrange(x, '(b f) c h w -> b c f h w', b=batch_size)
            ########################################### only change here, add camera_condition input ###########################################
            x = layer(x, context, camera_condition=camera_condition, additional_condition=additional_condition)
            ########################################### only change here, add camera_condition input ###########################################
            x = rearrange(x, 'b c f h w -> (b f) c h w')
        else:
            x = layer(x)
    return x


def new_forward_for_TemporalTransformer(self, x, context=None, camera_condition=None, additional_condition=None,):
    b, c, t, h, w = x.shape
    x_in = x
    x = self.norm(x)
    x = rearrange(x, 'b c t h w -> (b h w) c t').contiguous()
    if not self.use_linear:
        x = self.proj_in(x)
    x = rearrange(x, 'bhw c t -> bhw t c').contiguous()
    if self.use_linear:
        x = self.proj_in(x)

    temp_mask = None
    if self.causal_attention:
        # slice the from mask map
        temp_mask = self.mask[:, :t, :t].to(x.device)

    if temp_mask is not None:
        mask = temp_mask.to(x.device)
        mask = repeat(mask, 'l i j -> (l bhw) i j', bhw=b * h * w)
    else:
        mask = None

    if self.only_self_att:
        ## note: if no context is given, cross-attention defaults to self-attention
        for i, block in enumerate(self.transformer_blocks):
            ########################################### only change here, add camera_condition input ###########################################
            x = block(x, mask=mask, camera_condition=camera_condition, additional_condition=additional_condition)
            ########################################### only change here, add camera_condition input ###########################################
        x = rearrange(x, '(b hw) t c -> b hw t c', b=b).contiguous()
    else:
        x = rearrange(x, '(b hw) t c -> b hw t c', b=b).contiguous()
        context = rearrange(context, '(b t) l con -> b t l con', t=t).contiguous()
        for i, block in enumerate(self.transformer_blocks):
            # calculate each batch one by one (since number in shape could not greater then 65,535 for some package)
            for j in range(b):
                context_j = repeat(
                    context[j],
                    't l con -> (t r) l con', r=(h * w) // t, t=t).contiguous()
                ## note: causal mask will not applied in cross-attention case
                x[j] = block(x[j], context=context_j)

    if self.use_linear:
        x = self.proj_out(x)
        x = rearrange(x, 'b (h w) t c -> b c t h w', h=h, w=w).contiguous()
    if not self.use_linear:
        x = rearrange(x, 'b hw t c -> (b hw) c t').contiguous()
        x = self.proj_out(x)
        x = rearrange(x, '(b h w) c t -> b c t h w', b=b, h=h, w=w).contiguous()

    return x + x_in





# def new__forward_for_BasicTransformerBlock_of_SpatiallTransformer(self, x, context=None, mask=None, RT=None):
#     if isinstance(context, dict):
#         context = context['context']
#     x = self.attn1(self.norm1(x), context=context if self.disable_self_attn else None, mask=mask, RT=RT) + x
#     x = self.attn2(self.norm2(x), context=context, mask=mask, RT=RT) + x
#     x = self.ff(self.norm3(x)) + x
#     return x



def new_forward_for_BasicTransformerBlock_of_TemporalTransformer(self, x, context=None, mask=None, camera_condition=None, additional_condition=None, **kwargs):
    ## implementation tricks: because checkpointing doesn't support non-tensor (e.g. None or scalar) arguments
    forward_method = self._forward
    input_tuple = (x,)  ## should not be (x), otherwise *input_tuple will decouple x into multiple arguments

    if context is not None:
        input_tuple = (x, context)

    if mask is not None:
        forward_method = partial(forward_method, mask=mask)
    if camera_condition is not None:
        forward_method = partial(forward_method, camera_condition=camera_condition)
    if additional_condition is not None:
        forward_method = partial(forward_method, additional_condition=additional_condition)

    return checkpoint(forward_method, input_tuple, self.parameters(), self.checkpoint)


def new__forward_for_BasicTransformerBlock_of_TemporalTransformer(self, x, context=None, mask=None, camera_condition=None, additional_condition=None):

    # Add camera pose
    normed_x = self.norm1(x)
    if camera_condition is not None and isinstance(camera_condition, dict):
        pluker_embedding_features = camera_condition['pluker_embedding_features']
        zero_init_x = torch.zeros_like(normed_x)

        if hasattr(self, 'visual_cross_attn'):
            cross_out = self.visual_cross_attn(
                rearrange(normed_x, '(b h w) f c -> (b f) (h w) c', h=camera_condition['h'], w=camera_condition['w']),
                repeat(rearrange(additional_condition['condition_embedding'], 'B C F H W -> B (F H W) C'), 'B N C -> (B F) N C', F=normed_x.shape[1]),
                additional_condition["condition_mask"]
            )
            cross_out = rearrange(cross_out, '(b f) (h w) c -> (b h w) f c', h=camera_condition['h'], w=camera_condition['w'], f=normed_x.shape[1])
            zero_init_x = zero_init_x + cross_out

        if pluker_embedding_features is not None:
            pluker_embedding_features = rearrange(pluker_embedding_features, "b c f h w -> (b h w) f c")
            zero_init_x = zero_init_x + self.pluker_projection(normed_x + pluker_embedding_features)

        

        if hasattr(self, 'epipolar'):
            if pluker_embedding_features is not None:
                zero_init_x = zero_init_x + self.epipolar(
                    rearrange(normed_x + pluker_embedding_features, '(b h w) f c -> b f c h w', h=camera_condition['h'], w=camera_condition['w']),
                    **camera_condition,
                ) # (b h w) f c
            else:
                zero_init_x = zero_init_x + self.epipolar(
                    rearrange(normed_x, '(b h w) f c -> b f c h w', h=camera_condition['h'], w=camera_condition['w']),
                    **camera_condition,
                )  # (b h w) f c
        if 'add_type' in camera_condition and camera_condition['add_type'] == "add_to_main_branch":
            x = zero_init_x + self.attn1(normed_x, context=context if self.disable_self_attn else None, mask=mask) + x
        else:
            x = self.attn1(normed_x + zero_init_x, context=context if self.disable_self_attn else None, mask=mask) + x
    else:
        x = self.attn1(normed_x, context=context if self.disable_self_attn else None, mask=mask) + x
    x = self.attn2(self.norm2(x), context=context, mask=mask) + x
    x = self.ff(self.norm3(x)) + x
    return x