{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8f9e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'global_step': 50000,\n",
       "  'epoch': 51,\n",
       "  'dp_world_size': 8,\n",
       "  'mp_world_size': 1},\n",
       " 'state_dict': {'alphas_cumprod': {'param_count': 1000,\n",
       "   'shape': (1000,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2000,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'alphas_cumprod_prev': {'param_count': 1000,\n",
       "   'shape': (1000,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2000,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'betas': {'param_count': 1000,\n",
       "   'shape': (1000,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2000,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.ln_final.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.ln_final.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.logit_scale': {'param_count': 1,\n",
       "   'shape': (),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2,\n",
       "   'nbytes_human': '2.0 B'},\n",
       "  'cond_stage_model.model.positional_embedding': {'param_count': 78848,\n",
       "   'shape': (77, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 157696,\n",
       "   'nbytes_human': '154.0 KB'},\n",
       "  'cond_stage_model.model.text_projection': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.token_embedding.weight': {'param_count': 50593792,\n",
       "   'shape': (49408, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 101187584,\n",
       "   'nbytes_human': '96.5 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.attn.in_proj_bias': {'param_count': 3072,\n",
       "   'shape': (3072,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6144,\n",
       "   'nbytes_human': '6.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.attn.in_proj_weight': {'param_count': 3145728,\n",
       "   'shape': (3072, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6291456,\n",
       "   'nbytes_human': '6.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.attn.out_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.attn.out_proj.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.ln_1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.ln_1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.ln_2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.ln_2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.bias': {'param_count': 4096,\n",
       "   'shape': (4096,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8192,\n",
       "   'nbytes_human': '8.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'embedder.model.ln_final.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'embedder.model.ln_final.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'embedder.model.logit_scale': {'param_count': 1,\n",
       "   'shape': (),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2,\n",
       "   'nbytes_human': '2.0 B'},\n",
       "  'embedder.model.positional_embedding': {'param_count': 78848,\n",
       "   'shape': (77, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 157696,\n",
       "   'nbytes_human': '154.0 KB'},\n",
       "  'embedder.model.text_projection': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'embedder.model.token_embedding.weight': {'param_count': 50593792,\n",
       "   'shape': (49408, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 101187584,\n",
       "   'nbytes_human': '96.5 MB'},\n",
       "  'embedder.model.visual.class_embedding': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.conv1.weight': {'param_count': 752640,\n",
       "   'shape': (1280, 3, 14, 14),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1505280,\n",
       "   'nbytes_human': '1.4 MB'},\n",
       "  'embedder.model.visual.ln_post.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.ln_post.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.ln_pre.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.ln_pre.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.positional_embedding': {'param_count': 328960,\n",
       "   'shape': (257, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 657920,\n",
       "   'nbytes_human': '642.5 KB'},\n",
       "  'embedder.model.visual.proj': {'param_count': 1310720,\n",
       "   'shape': (1280, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2621440,\n",
       "   'nbytes_human': '2.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.0.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.1.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.10.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.11.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.12.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.13.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.14.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.15.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.16.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.17.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.18.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.19.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.2.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.20.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.21.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.22.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.23.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.24.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.25.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.26.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.27.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.28.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.29.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.3.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.30.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.31.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.4.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.5.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.6.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.7.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.8.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.attn.in_proj_bias': {'param_count': 3840,\n",
       "   'shape': (3840,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 7680,\n",
       "   'nbytes_human': '7.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.attn.in_proj_weight': {'param_count': 4915200,\n",
       "   'shape': (3840, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 9830400,\n",
       "   'nbytes_human': '9.4 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.attn.out_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.attn.out_proj.weight': {'param_count': 1638400,\n",
       "   'shape': (1280, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3276800,\n",
       "   'nbytes_human': '3.1 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.ln_1.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.ln_1.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.ln_2.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.ln_2.weight': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.mlp.c_fc.bias': {'param_count': 5120,\n",
       "   'shape': (5120,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 10240,\n",
       "   'nbytes_human': '10.0 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.mlp.c_fc.weight': {'param_count': 6553600,\n",
       "   'shape': (5120, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.mlp.c_proj.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  'embedder.model.visual.transformer.resblocks.9.mlp.c_proj.weight': {'param_count': 6553600,\n",
       "   'shape': (1280, 5120),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 13107200,\n",
       "   'nbytes_human': '12.5 MB'},\n",
       "  'first_stage_model.decoder.conv_in.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.conv_in.weight': {'param_count': 18432,\n",
       "   'shape': (512, 4, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 36864,\n",
       "   'nbytes_human': '36.0 KB'},\n",
       "  'first_stage_model.decoder.conv_out.bias': {'param_count': 3,\n",
       "   'shape': (3,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6,\n",
       "   'nbytes_human': '6.0 B'},\n",
       "  'first_stage_model.decoder.conv_out.weight': {'param_count': 3456,\n",
       "   'shape': (3, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6912,\n",
       "   'nbytes_human': '6.8 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.k.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.k.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.norm.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.norm.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.proj_out.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.proj_out.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.q.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.q.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.v.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.attn_1.v.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_1.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_1.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.mid.block_1.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_1.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.mid.block_1.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_1.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_1.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_1.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_2.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_2.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.mid.block_2.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_2.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.mid.block_2.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_2.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_2.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.mid.block_2.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.norm_out.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.norm_out.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.0.conv1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.0.conv1.weight': {'param_count': 294912,\n",
       "   'shape': (128, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 589824,\n",
       "   'nbytes_human': '576.0 KB'},\n",
       "  'first_stage_model.decoder.up.0.block.0.conv2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.0.conv2.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight': {'param_count': 32768,\n",
       "   'shape': (128, 256, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 65536,\n",
       "   'nbytes_human': '64.0 KB'},\n",
       "  'first_stage_model.decoder.up.0.block.0.norm1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.0.norm1.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.0.norm2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.0.norm2.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.1.conv1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.1.conv1.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.decoder.up.0.block.1.conv2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.1.conv2.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.decoder.up.0.block.1.norm1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.1.norm1.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.1.norm2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.1.norm2.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.2.conv1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.2.conv1.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.decoder.up.0.block.2.conv2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.2.conv2.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.decoder.up.0.block.2.norm1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.2.norm1.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.2.norm2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.0.block.2.norm2.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.0.conv1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.0.conv1.weight': {'param_count': 1179648,\n",
       "   'shape': (256, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2359296,\n",
       "   'nbytes_human': '2.2 MB'},\n",
       "  'first_stage_model.decoder.up.1.block.0.conv2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.0.conv2.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight': {'param_count': 131072,\n",
       "   'shape': (256, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 262144,\n",
       "   'nbytes_human': '256.0 KB'},\n",
       "  'first_stage_model.decoder.up.1.block.0.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.1.block.0.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.1.block.0.norm2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.0.norm2.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.1.conv1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.1.conv1.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.decoder.up.1.block.1.conv2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.1.conv2.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.decoder.up.1.block.1.norm1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.1.norm1.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.1.norm2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.1.norm2.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.2.conv1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.2.conv1.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.decoder.up.1.block.2.conv2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.2.conv2.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.decoder.up.1.block.2.norm1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.2.norm1.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.2.norm2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.block.2.norm2.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.upsample.conv.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.decoder.up.1.upsample.conv.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.0.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.1.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.block.2.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.upsample.conv.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.2.upsample.conv.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.0.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.1.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.block.2.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.upsample.conv.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.decoder.up.3.upsample.conv.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.conv_in.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.conv_in.weight': {'param_count': 3456,\n",
       "   'shape': (128, 3, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 6912,\n",
       "   'nbytes_human': '6.8 KB'},\n",
       "  'first_stage_model.encoder.conv_out.bias': {'param_count': 8,\n",
       "   'shape': (8,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 16,\n",
       "   'nbytes_human': '16.0 B'},\n",
       "  'first_stage_model.encoder.conv_out.weight': {'param_count': 36864,\n",
       "   'shape': (8, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 73728,\n",
       "   'nbytes_human': '72.0 KB'},\n",
       "  'first_stage_model.encoder.down.0.block.0.conv1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.0.conv1.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.encoder.down.0.block.0.conv2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.0.conv2.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.encoder.down.0.block.0.norm1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.0.norm1.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.0.norm2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.0.norm2.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.1.conv1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.1.conv1.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.encoder.down.0.block.1.conv2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.1.conv2.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.encoder.down.0.block.1.norm1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.1.norm1.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.1.norm2.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.block.1.norm2.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.downsample.conv.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.0.downsample.conv.weight': {'param_count': 147456,\n",
       "   'shape': (128, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 294912,\n",
       "   'nbytes_human': '288.0 KB'},\n",
       "  'first_stage_model.encoder.down.1.block.0.conv1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.0.conv1.weight': {'param_count': 294912,\n",
       "   'shape': (256, 128, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 589824,\n",
       "   'nbytes_human': '576.0 KB'},\n",
       "  'first_stage_model.encoder.down.1.block.0.conv2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.0.conv2.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight': {'param_count': 32768,\n",
       "   'shape': (256, 128, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 65536,\n",
       "   'nbytes_human': '64.0 KB'},\n",
       "  'first_stage_model.encoder.down.1.block.0.norm1.bias': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.0.norm1.weight': {'param_count': 128,\n",
       "   'shape': (128,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 256,\n",
       "   'nbytes_human': '256.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.0.norm2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.0.norm2.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.1.conv1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.1.conv1.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.encoder.down.1.block.1.conv2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.1.conv2.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.encoder.down.1.block.1.norm1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.1.norm1.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.1.norm2.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.block.1.norm2.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.downsample.conv.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.1.downsample.conv.weight': {'param_count': 589824,\n",
       "   'shape': (256, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1179648,\n",
       "   'nbytes_human': '1.1 MB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.conv1.weight': {'param_count': 1179648,\n",
       "   'shape': (512, 256, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2359296,\n",
       "   'nbytes_human': '2.2 MB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight': {'param_count': 131072,\n",
       "   'shape': (512, 256, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 262144,\n",
       "   'nbytes_human': '256.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.norm1.bias': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.2.block.0.norm1.weight': {'param_count': 256,\n",
       "   'shape': (256,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 512,\n",
       "   'nbytes_human': '512.0 B'},\n",
       "  'first_stage_model.encoder.down.2.block.0.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.0.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.block.1.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.downsample.conv.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.2.downsample.conv.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.0.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.down.3.block.1.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.k.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.k.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.norm.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.norm.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.proj_out.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.proj_out.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.q.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.q.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.v.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.attn_1.v.weight': {'param_count': 262144,\n",
       "   'shape': (512, 512, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_1.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_1.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.mid.block_1.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_1.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.mid.block_1.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_1.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_1.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_1.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_2.conv1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_2.conv1.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.mid.block_2.conv2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_2.conv2.weight': {'param_count': 2359296,\n",
       "   'shape': (512, 512, 3, 3),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 4718592,\n",
       "   'nbytes_human': '4.5 MB'},\n",
       "  'first_stage_model.encoder.mid.block_2.norm1.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_2.norm1.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_2.norm2.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.mid.block_2.norm2.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.norm_out.bias': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.encoder.norm_out.weight': {'param_count': 512,\n",
       "   'shape': (512,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1024,\n",
       "   'nbytes_human': '1.0 KB'},\n",
       "  'first_stage_model.post_quant_conv.bias': {'param_count': 4,\n",
       "   'shape': (4,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8,\n",
       "   'nbytes_human': '8.0 B'},\n",
       "  'first_stage_model.post_quant_conv.weight': {'param_count': 16,\n",
       "   'shape': (4, 4, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 32,\n",
       "   'nbytes_human': '32.0 B'},\n",
       "  'first_stage_model.quant_conv.bias': {'param_count': 8,\n",
       "   'shape': (8,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 16,\n",
       "   'nbytes_human': '16.0 B'},\n",
       "  'first_stage_model.quant_conv.weight': {'param_count': 64,\n",
       "   'shape': (8, 8, 1, 1),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 128,\n",
       "   'nbytes_human': '128.0 B'},\n",
       "  'image_proj_model.latents': {'param_count': 262144,\n",
       "   'shape': (1, 256, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 524288,\n",
       "   'nbytes_human': '512.0 KB'},\n",
       "  'image_proj_model.layers.0.0.norm1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.0.0.norm1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.0.0.norm2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.0.0.norm2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.0.0.to_kv.weight': {'param_count': 1572864,\n",
       "   'shape': (1536, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3145728,\n",
       "   'nbytes_human': '3.0 MB'},\n",
       "  'image_proj_model.layers.0.0.to_out.weight': {'param_count': 786432,\n",
       "   'shape': (1024, 768),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.0.0.to_q.weight': {'param_count': 786432,\n",
       "   'shape': (768, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.0.1.0.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.0.1.0.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.0.1.1.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.layers.0.1.3.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.layers.1.0.norm1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.1.0.norm1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.1.0.norm2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.1.0.norm2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.1.0.to_kv.weight': {'param_count': 1572864,\n",
       "   'shape': (1536, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3145728,\n",
       "   'nbytes_human': '3.0 MB'},\n",
       "  'image_proj_model.layers.1.0.to_out.weight': {'param_count': 786432,\n",
       "   'shape': (1024, 768),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.1.0.to_q.weight': {'param_count': 786432,\n",
       "   'shape': (768, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.1.1.0.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.1.1.0.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.1.1.1.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.layers.1.1.3.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.layers.2.0.norm1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.2.0.norm1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.2.0.norm2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.2.0.norm2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.2.0.to_kv.weight': {'param_count': 1572864,\n",
       "   'shape': (1536, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3145728,\n",
       "   'nbytes_human': '3.0 MB'},\n",
       "  'image_proj_model.layers.2.0.to_out.weight': {'param_count': 786432,\n",
       "   'shape': (1024, 768),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.2.0.to_q.weight': {'param_count': 786432,\n",
       "   'shape': (768, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.2.1.0.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.2.1.0.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.2.1.1.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.layers.2.1.3.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.layers.3.0.norm1.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.3.0.norm1.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.3.0.norm2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.3.0.norm2.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.3.0.to_kv.weight': {'param_count': 1572864,\n",
       "   'shape': (1536, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 3145728,\n",
       "   'nbytes_human': '3.0 MB'},\n",
       "  'image_proj_model.layers.3.0.to_out.weight': {'param_count': 786432,\n",
       "   'shape': (1024, 768),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.3.0.to_q.weight': {'param_count': 786432,\n",
       "   'shape': (768, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 1572864,\n",
       "   'nbytes_human': '1.5 MB'},\n",
       "  'image_proj_model.layers.3.1.0.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.3.1.0.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.layers.3.1.1.weight': {'param_count': 4194304,\n",
       "   'shape': (4096, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.layers.3.1.3.weight': {'param_count': 4194304,\n",
       "   'shape': (1024, 4096),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 8388608,\n",
       "   'nbytes_human': '8.0 MB'},\n",
       "  'image_proj_model.norm_out.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.norm_out.weight': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.proj_in.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.proj_in.weight': {'param_count': 1310720,\n",
       "   'shape': (1024, 1280),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2621440,\n",
       "   'nbytes_human': '2.5 MB'},\n",
       "  'image_proj_model.proj_out.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.proj_out.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'image_proj_model.timestep_embedding_func.0.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.timestep_embedding_func.0.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'image_proj_model.timestep_embedding_func.2.bias': {'param_count': 1024,\n",
       "   'shape': (1024,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2048,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'image_proj_model.timestep_embedding_func.2.weight': {'param_count': 1048576,\n",
       "   'shape': (1024, 1024),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2097152,\n",
       "   'nbytes_human': '2.0 MB'},\n",
       "  'log_one_minus_alphas_cumprod': {'param_count': 1000,\n",
       "   'shape': (1000,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2000,\n",
       "   'nbytes_human': '2.0 KB'},\n",
       "  'model.diffusion_model.fps_embedding.0.bias': {'param_count': 1280,\n",
       "   'shape': (1280,),\n",
       "   'dtype': 'torch.float16',\n",
       "   'device': 'cpu',\n",
       "   'requires_grad': False,\n",
       "   'type': 'torch.Tensor',\n",
       "   'nbytes': 2560,\n",
       "   'nbytes_human': '2.5 KB'},\n",
       "  ...},\n",
       " 'totals': {'total_parameters': 2919094813,\n",
       "  'total_trainable_parameters': 0,\n",
       "  'total_memory_bytes': 5838189626,\n",
       "  'total_memory_human': '5.4 GB'},\n",
       " 'options': {'filter_regex': None,\n",
       "  'only_trainable': False,\n",
       "  'include_non_arrays': True,\n",
       "  'limit': None,\n",
       "  'sort_by': 'name',\n",
       "  'descending': False,\n",
       "  'add_bytes_column': False,\n",
       "  'human_readable_bytes': True,\n",
       "  'include_model_summary': True,\n",
       "  'tablefmt': 'plain',\n",
       "  'map_location': 'cpu'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from VidUtil.torch_utils import inspect_checkpoint\n",
    "\n",
    "checkpoint_path = \"/mnt/Data/denninge/CamI2V/results/multi_true_l2_20250226_080151/checkpoints/trainstep_checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\"\n",
    "checkpoint_info = inspect_checkpoint(checkpoint_path, return_dict=True)\n",
    "checkpoint_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a2197c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/denninge/anaconda3/envs/cami2v/lib/python3.10/site-packages/torch/cuda/__init__.py:58: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "/tmp/ipykernel_364425/2257491538.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "checkpoint_path = \"/mnt/Data/denninge/CamI2V/results/multi_true_l2_20250226_080151/checkpoints/trainstep_checkpoints/last.ckpt/checkpoint/mp_rank_00_model_states.pt\"\n",
    "ckpt = torch.load(checkpoint_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9885bea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['betas', 'alphas_cumprod', 'alphas_cumprod_prev', 'sqrt_alphas_cumprod', 'sqrt_one_minus_alphas_cumprod', 'log_one_minus_alphas_cumprod', 'sqrt_recip_alphas_cumprod', 'sqrt_recipm1_alphas_cumprod', 'posterior_variance', 'posterior_log_variance_clipped', 'posterior_mean_coef1', 'posterior_mean_coef2', 'model.diffusion_model.time_embed.0.weight', 'model.diffusion_model.time_embed.0.bias', 'model.diffusion_model.time_embed.2.weight', 'model.diffusion_model.time_embed.2.bias', 'model.diffusion_model.fps_embedding.0.weight', 'model.diffusion_model.fps_embedding.0.bias', 'model.diffusion_model.fps_embedding.2.weight', 'model.diffusion_model.fps_embedding.2.bias', 'model.diffusion_model.input_blocks.0.0.weight', 'model.diffusion_model.input_blocks.0.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.input_blocks.1.1.norm.weight', 'model.diffusion_model.input_blocks.1.1.norm.bias', 'model.diffusion_model.input_blocks.1.1.proj_in.weight', 'model.diffusion_model.input_blocks.1.1.proj_in.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.1.proj_out.weight', 'model.diffusion_model.input_blocks.1.1.proj_out.bias', 'model.diffusion_model.input_blocks.1.2.norm.weight', 'model.diffusion_model.input_blocks.1.2.norm.bias', 'model.diffusion_model.input_blocks.1.2.proj_in.weight', 'model.diffusion_model.input_blocks.1.2.proj_in.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.input_blocks.1.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.input_blocks.1.2.proj_out.weight', 'model.diffusion_model.input_blocks.1.2.proj_out.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.input_blocks.2.1.norm.weight', 'model.diffusion_model.input_blocks.2.1.norm.bias', 'model.diffusion_model.input_blocks.2.1.proj_in.weight', 'model.diffusion_model.input_blocks.2.1.proj_in.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.1.proj_out.weight', 'model.diffusion_model.input_blocks.2.1.proj_out.bias', 'model.diffusion_model.input_blocks.2.2.norm.weight', 'model.diffusion_model.input_blocks.2.2.norm.bias', 'model.diffusion_model.input_blocks.2.2.proj_in.weight', 'model.diffusion_model.input_blocks.2.2.proj_in.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.input_blocks.2.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.input_blocks.2.2.proj_out.weight', 'model.diffusion_model.input_blocks.2.2.proj_out.bias', 'model.diffusion_model.input_blocks.3.0.op.weight', 'model.diffusion_model.input_blocks.3.0.op.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.4.0.skip_connection.weight', 'model.diffusion_model.input_blocks.4.0.skip_connection.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.input_blocks.4.1.norm.weight', 'model.diffusion_model.input_blocks.4.1.norm.bias', 'model.diffusion_model.input_blocks.4.1.proj_in.weight', 'model.diffusion_model.input_blocks.4.1.proj_in.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.1.proj_out.weight', 'model.diffusion_model.input_blocks.4.1.proj_out.bias', 'model.diffusion_model.input_blocks.4.2.norm.weight', 'model.diffusion_model.input_blocks.4.2.norm.bias', 'model.diffusion_model.input_blocks.4.2.proj_in.weight', 'model.diffusion_model.input_blocks.4.2.proj_in.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.input_blocks.4.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.input_blocks.4.2.proj_out.weight', 'model.diffusion_model.input_blocks.4.2.proj_out.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.input_blocks.5.1.norm.weight', 'model.diffusion_model.input_blocks.5.1.norm.bias', 'model.diffusion_model.input_blocks.5.1.proj_in.weight', 'model.diffusion_model.input_blocks.5.1.proj_in.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.1.proj_out.weight', 'model.diffusion_model.input_blocks.5.1.proj_out.bias', 'model.diffusion_model.input_blocks.5.2.norm.weight', 'model.diffusion_model.input_blocks.5.2.norm.bias', 'model.diffusion_model.input_blocks.5.2.proj_in.weight', 'model.diffusion_model.input_blocks.5.2.proj_in.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.input_blocks.5.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.input_blocks.5.2.proj_out.weight', 'model.diffusion_model.input_blocks.5.2.proj_out.bias', 'model.diffusion_model.input_blocks.6.0.op.weight', 'model.diffusion_model.input_blocks.6.0.op.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.7.0.skip_connection.weight', 'model.diffusion_model.input_blocks.7.0.skip_connection.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.input_blocks.7.1.norm.weight', 'model.diffusion_model.input_blocks.7.1.norm.bias', 'model.diffusion_model.input_blocks.7.1.proj_in.weight', 'model.diffusion_model.input_blocks.7.1.proj_in.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.1.proj_out.weight', 'model.diffusion_model.input_blocks.7.1.proj_out.bias', 'model.diffusion_model.input_blocks.7.2.norm.weight', 'model.diffusion_model.input_blocks.7.2.norm.bias', 'model.diffusion_model.input_blocks.7.2.proj_in.weight', 'model.diffusion_model.input_blocks.7.2.proj_in.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.input_blocks.7.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.input_blocks.7.2.proj_out.weight', 'model.diffusion_model.input_blocks.7.2.proj_out.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.input_blocks.8.1.norm.weight', 'model.diffusion_model.input_blocks.8.1.norm.bias', 'model.diffusion_model.input_blocks.8.1.proj_in.weight', 'model.diffusion_model.input_blocks.8.1.proj_in.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.1.proj_out.weight', 'model.diffusion_model.input_blocks.8.1.proj_out.bias', 'model.diffusion_model.input_blocks.8.2.norm.weight', 'model.diffusion_model.input_blocks.8.2.norm.bias', 'model.diffusion_model.input_blocks.8.2.proj_in.weight', 'model.diffusion_model.input_blocks.8.2.proj_in.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.input_blocks.8.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.input_blocks.8.2.proj_out.weight', 'model.diffusion_model.input_blocks.8.2.proj_out.bias', 'model.diffusion_model.input_blocks.9.0.op.weight', 'model.diffusion_model.input_blocks.9.0.op.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.input_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.input_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.input_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.input_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.init_attn.0.norm.weight', 'model.diffusion_model.init_attn.0.norm.bias', 'model.diffusion_model.init_attn.0.proj_in.weight', 'model.diffusion_model.init_attn.0.proj_in.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.norm1.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.norm1.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.norm2.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.norm2.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.norm3.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.norm3.bias', 'model.diffusion_model.init_attn.0.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.init_attn.0.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.init_attn.0.proj_out.weight', 'model.diffusion_model.init_attn.0.proj_out.bias', 'model.diffusion_model.middle_block.0.in_layers.0.weight', 'model.diffusion_model.middle_block.0.in_layers.0.bias', 'model.diffusion_model.middle_block.0.in_layers.2.weight', 'model.diffusion_model.middle_block.0.in_layers.2.bias', 'model.diffusion_model.middle_block.0.emb_layers.1.weight', 'model.diffusion_model.middle_block.0.emb_layers.1.bias', 'model.diffusion_model.middle_block.0.out_layers.0.weight', 'model.diffusion_model.middle_block.0.out_layers.0.bias', 'model.diffusion_model.middle_block.0.out_layers.3.weight', 'model.diffusion_model.middle_block.0.out_layers.3.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.middle_block.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.middle_block.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.middle_block.1.norm.weight', 'model.diffusion_model.middle_block.1.norm.bias', 'model.diffusion_model.middle_block.1.proj_in.weight', 'model.diffusion_model.middle_block.1.proj_in.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.1.proj_out.weight', 'model.diffusion_model.middle_block.1.proj_out.bias', 'model.diffusion_model.middle_block.2.norm.weight', 'model.diffusion_model.middle_block.2.norm.bias', 'model.diffusion_model.middle_block.2.proj_in.weight', 'model.diffusion_model.middle_block.2.proj_in.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.middle_block.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.middle_block.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.middle_block.2.proj_out.weight', 'model.diffusion_model.middle_block.2.proj_out.bias', 'model.diffusion_model.middle_block.3.in_layers.0.weight', 'model.diffusion_model.middle_block.3.in_layers.0.bias', 'model.diffusion_model.middle_block.3.in_layers.2.weight', 'model.diffusion_model.middle_block.3.in_layers.2.bias', 'model.diffusion_model.middle_block.3.emb_layers.1.weight', 'model.diffusion_model.middle_block.3.emb_layers.1.bias', 'model.diffusion_model.middle_block.3.out_layers.0.weight', 'model.diffusion_model.middle_block.3.out_layers.0.bias', 'model.diffusion_model.middle_block.3.out_layers.3.weight', 'model.diffusion_model.middle_block.3.out_layers.3.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv1.0.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv1.0.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv1.2.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv1.2.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv2.0.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv2.0.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv2.3.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv2.3.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv3.0.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv3.0.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv3.3.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv3.3.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv4.0.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv4.0.bias', 'model.diffusion_model.middle_block.3.temopral_conv.conv4.3.weight', 'model.diffusion_model.middle_block.3.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.0.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.0.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.0.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.0.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.0.0.skip_connection.weight', 'model.diffusion_model.output_blocks.0.0.skip_connection.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.1.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.1.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.1.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.1.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.1.0.skip_connection.weight', 'model.diffusion_model.output_blocks.1.0.skip_connection.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.2.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.2.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.2.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.2.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.2.0.skip_connection.weight', 'model.diffusion_model.output_blocks.2.0.skip_connection.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.2.1.conv.weight', 'model.diffusion_model.output_blocks.2.1.conv.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.3.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.3.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.3.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.3.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.3.0.skip_connection.weight', 'model.diffusion_model.output_blocks.3.0.skip_connection.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.3.1.norm.weight', 'model.diffusion_model.output_blocks.3.1.norm.bias', 'model.diffusion_model.output_blocks.3.1.proj_in.weight', 'model.diffusion_model.output_blocks.3.1.proj_in.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.1.proj_out.weight', 'model.diffusion_model.output_blocks.3.1.proj_out.bias', 'model.diffusion_model.output_blocks.3.2.norm.weight', 'model.diffusion_model.output_blocks.3.2.norm.bias', 'model.diffusion_model.output_blocks.3.2.proj_in.weight', 'model.diffusion_model.output_blocks.3.2.proj_in.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.3.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.3.2.proj_out.weight', 'model.diffusion_model.output_blocks.3.2.proj_out.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.4.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.4.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.4.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.4.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.4.0.skip_connection.weight', 'model.diffusion_model.output_blocks.4.0.skip_connection.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.4.1.norm.weight', 'model.diffusion_model.output_blocks.4.1.norm.bias', 'model.diffusion_model.output_blocks.4.1.proj_in.weight', 'model.diffusion_model.output_blocks.4.1.proj_in.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.1.proj_out.weight', 'model.diffusion_model.output_blocks.4.1.proj_out.bias', 'model.diffusion_model.output_blocks.4.2.norm.weight', 'model.diffusion_model.output_blocks.4.2.norm.bias', 'model.diffusion_model.output_blocks.4.2.proj_in.weight', 'model.diffusion_model.output_blocks.4.2.proj_in.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.4.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.4.2.proj_out.weight', 'model.diffusion_model.output_blocks.4.2.proj_out.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.5.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.5.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.5.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.5.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.5.0.skip_connection.weight', 'model.diffusion_model.output_blocks.5.0.skip_connection.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.5.1.norm.weight', 'model.diffusion_model.output_blocks.5.1.norm.bias', 'model.diffusion_model.output_blocks.5.1.proj_in.weight', 'model.diffusion_model.output_blocks.5.1.proj_in.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.1.proj_out.weight', 'model.diffusion_model.output_blocks.5.1.proj_out.bias', 'model.diffusion_model.output_blocks.5.2.norm.weight', 'model.diffusion_model.output_blocks.5.2.norm.bias', 'model.diffusion_model.output_blocks.5.2.proj_in.weight', 'model.diffusion_model.output_blocks.5.2.proj_in.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.5.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.5.2.proj_out.weight', 'model.diffusion_model.output_blocks.5.2.proj_out.bias', 'model.diffusion_model.output_blocks.5.3.conv.weight', 'model.diffusion_model.output_blocks.5.3.conv.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.6.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.6.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.6.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.6.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.6.0.skip_connection.weight', 'model.diffusion_model.output_blocks.6.0.skip_connection.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.6.1.norm.weight', 'model.diffusion_model.output_blocks.6.1.norm.bias', 'model.diffusion_model.output_blocks.6.1.proj_in.weight', 'model.diffusion_model.output_blocks.6.1.proj_in.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.1.proj_out.weight', 'model.diffusion_model.output_blocks.6.1.proj_out.bias', 'model.diffusion_model.output_blocks.6.2.norm.weight', 'model.diffusion_model.output_blocks.6.2.norm.bias', 'model.diffusion_model.output_blocks.6.2.proj_in.weight', 'model.diffusion_model.output_blocks.6.2.proj_in.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.6.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.6.2.proj_out.weight', 'model.diffusion_model.output_blocks.6.2.proj_out.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.7.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.7.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.7.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.7.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.7.0.skip_connection.weight', 'model.diffusion_model.output_blocks.7.0.skip_connection.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.7.1.norm.weight', 'model.diffusion_model.output_blocks.7.1.norm.bias', 'model.diffusion_model.output_blocks.7.1.proj_in.weight', 'model.diffusion_model.output_blocks.7.1.proj_in.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.1.proj_out.weight', 'model.diffusion_model.output_blocks.7.1.proj_out.bias', 'model.diffusion_model.output_blocks.7.2.norm.weight', 'model.diffusion_model.output_blocks.7.2.norm.bias', 'model.diffusion_model.output_blocks.7.2.proj_in.weight', 'model.diffusion_model.output_blocks.7.2.proj_in.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.7.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.7.2.proj_out.weight', 'model.diffusion_model.output_blocks.7.2.proj_out.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.8.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.8.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.8.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.8.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.8.0.skip_connection.weight', 'model.diffusion_model.output_blocks.8.0.skip_connection.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.8.1.norm.weight', 'model.diffusion_model.output_blocks.8.1.norm.bias', 'model.diffusion_model.output_blocks.8.1.proj_in.weight', 'model.diffusion_model.output_blocks.8.1.proj_in.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.1.proj_out.weight', 'model.diffusion_model.output_blocks.8.1.proj_out.bias', 'model.diffusion_model.output_blocks.8.2.norm.weight', 'model.diffusion_model.output_blocks.8.2.norm.bias', 'model.diffusion_model.output_blocks.8.2.proj_in.weight', 'model.diffusion_model.output_blocks.8.2.proj_in.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.8.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.8.2.proj_out.weight', 'model.diffusion_model.output_blocks.8.2.proj_out.bias', 'model.diffusion_model.output_blocks.8.3.conv.weight', 'model.diffusion_model.output_blocks.8.3.conv.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.9.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.9.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.9.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.9.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.9.0.skip_connection.weight', 'model.diffusion_model.output_blocks.9.0.skip_connection.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.9.1.norm.weight', 'model.diffusion_model.output_blocks.9.1.norm.bias', 'model.diffusion_model.output_blocks.9.1.proj_in.weight', 'model.diffusion_model.output_blocks.9.1.proj_in.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.1.proj_out.weight', 'model.diffusion_model.output_blocks.9.1.proj_out.bias', 'model.diffusion_model.output_blocks.9.2.norm.weight', 'model.diffusion_model.output_blocks.9.2.norm.bias', 'model.diffusion_model.output_blocks.9.2.proj_in.weight', 'model.diffusion_model.output_blocks.9.2.proj_in.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.9.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.9.2.proj_out.weight', 'model.diffusion_model.output_blocks.9.2.proj_out.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.10.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.10.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.10.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.10.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.10.0.skip_connection.weight', 'model.diffusion_model.output_blocks.10.0.skip_connection.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.10.1.norm.weight', 'model.diffusion_model.output_blocks.10.1.norm.bias', 'model.diffusion_model.output_blocks.10.1.proj_in.weight', 'model.diffusion_model.output_blocks.10.1.proj_in.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.1.proj_out.weight', 'model.diffusion_model.output_blocks.10.1.proj_out.bias', 'model.diffusion_model.output_blocks.10.2.norm.weight', 'model.diffusion_model.output_blocks.10.2.norm.bias', 'model.diffusion_model.output_blocks.10.2.proj_in.weight', 'model.diffusion_model.output_blocks.10.2.proj_in.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.10.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.10.2.proj_out.weight', 'model.diffusion_model.output_blocks.10.2.proj_out.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.in_layers.2.weight', 'model.diffusion_model.output_blocks.11.0.in_layers.2.bias', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.weight', 'model.diffusion_model.output_blocks.11.0.emb_layers.1.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.0.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.0.bias', 'model.diffusion_model.output_blocks.11.0.out_layers.3.weight', 'model.diffusion_model.output_blocks.11.0.out_layers.3.bias', 'model.diffusion_model.output_blocks.11.0.skip_connection.weight', 'model.diffusion_model.output_blocks.11.0.skip_connection.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.0.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.0.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.2.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.2.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.0.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.0.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.3.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.3.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.0.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.0.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.3.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.3.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.0.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.0.bias', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.3.weight', 'model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.3.bias', 'model.diffusion_model.output_blocks.11.1.norm.weight', 'model.diffusion_model.output_blocks.11.1.norm.bias', 'model.diffusion_model.output_blocks.11.1.proj_in.weight', 'model.diffusion_model.output_blocks.11.1.proj_in.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.alpha', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k_ip.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v_ip.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.1.proj_out.weight', 'model.diffusion_model.output_blocks.11.1.proj_out.bias', 'model.diffusion_model.output_blocks.11.2.norm.weight', 'model.diffusion_model.output_blocks.11.2.norm.bias', 'model.diffusion_model.output_blocks.11.2.proj_in.weight', 'model.diffusion_model.output_blocks.11.2.proj_in.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_q.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_k.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_v.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_out.0.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_out.0.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.0.proj.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.0.proj.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.2.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.2.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_q.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_k.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_v.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_out.0.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_out.0.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm1.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm1.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm2.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm2.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm3.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm3.bias', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.cc_projection.weight', 'model.diffusion_model.output_blocks.11.2.transformer_blocks.0.cc_projection.bias', 'model.diffusion_model.output_blocks.11.2.proj_out.weight', 'model.diffusion_model.output_blocks.11.2.proj_out.bias', 'model.diffusion_model.out.0.weight', 'model.diffusion_model.out.0.bias', 'model.diffusion_model.out.2.weight', 'model.diffusion_model.out.2.bias', 'first_stage_model.encoder.conv_in.weight', 'first_stage_model.encoder.conv_in.bias', 'first_stage_model.encoder.down.0.block.0.norm1.weight', 'first_stage_model.encoder.down.0.block.0.norm1.bias', 'first_stage_model.encoder.down.0.block.0.conv1.weight', 'first_stage_model.encoder.down.0.block.0.conv1.bias', 'first_stage_model.encoder.down.0.block.0.norm2.weight', 'first_stage_model.encoder.down.0.block.0.norm2.bias', 'first_stage_model.encoder.down.0.block.0.conv2.weight', 'first_stage_model.encoder.down.0.block.0.conv2.bias', 'first_stage_model.encoder.down.0.block.1.norm1.weight', 'first_stage_model.encoder.down.0.block.1.norm1.bias', 'first_stage_model.encoder.down.0.block.1.conv1.weight', 'first_stage_model.encoder.down.0.block.1.conv1.bias', 'first_stage_model.encoder.down.0.block.1.norm2.weight', 'first_stage_model.encoder.down.0.block.1.norm2.bias', 'first_stage_model.encoder.down.0.block.1.conv2.weight', 'first_stage_model.encoder.down.0.block.1.conv2.bias', 'first_stage_model.encoder.down.0.downsample.conv.weight', 'first_stage_model.encoder.down.0.downsample.conv.bias', 'first_stage_model.encoder.down.1.block.0.norm1.weight', 'first_stage_model.encoder.down.1.block.0.norm1.bias', 'first_stage_model.encoder.down.1.block.0.conv1.weight', 'first_stage_model.encoder.down.1.block.0.conv1.bias', 'first_stage_model.encoder.down.1.block.0.norm2.weight', 'first_stage_model.encoder.down.1.block.0.norm2.bias', 'first_stage_model.encoder.down.1.block.0.conv2.weight', 'first_stage_model.encoder.down.1.block.0.conv2.bias', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.1.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.1.block.1.norm1.weight', 'first_stage_model.encoder.down.1.block.1.norm1.bias', 'first_stage_model.encoder.down.1.block.1.conv1.weight', 'first_stage_model.encoder.down.1.block.1.conv1.bias', 'first_stage_model.encoder.down.1.block.1.norm2.weight', 'first_stage_model.encoder.down.1.block.1.norm2.bias', 'first_stage_model.encoder.down.1.block.1.conv2.weight', 'first_stage_model.encoder.down.1.block.1.conv2.bias', 'first_stage_model.encoder.down.1.downsample.conv.weight', 'first_stage_model.encoder.down.1.downsample.conv.bias', 'first_stage_model.encoder.down.2.block.0.norm1.weight', 'first_stage_model.encoder.down.2.block.0.norm1.bias', 'first_stage_model.encoder.down.2.block.0.conv1.weight', 'first_stage_model.encoder.down.2.block.0.conv1.bias', 'first_stage_model.encoder.down.2.block.0.norm2.weight', 'first_stage_model.encoder.down.2.block.0.norm2.bias', 'first_stage_model.encoder.down.2.block.0.conv2.weight', 'first_stage_model.encoder.down.2.block.0.conv2.bias', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.weight', 'first_stage_model.encoder.down.2.block.0.nin_shortcut.bias', 'first_stage_model.encoder.down.2.block.1.norm1.weight', 'first_stage_model.encoder.down.2.block.1.norm1.bias', 'first_stage_model.encoder.down.2.block.1.conv1.weight', 'first_stage_model.encoder.down.2.block.1.conv1.bias', 'first_stage_model.encoder.down.2.block.1.norm2.weight', 'first_stage_model.encoder.down.2.block.1.norm2.bias', 'first_stage_model.encoder.down.2.block.1.conv2.weight', 'first_stage_model.encoder.down.2.block.1.conv2.bias', 'first_stage_model.encoder.down.2.downsample.conv.weight', 'first_stage_model.encoder.down.2.downsample.conv.bias', 'first_stage_model.encoder.down.3.block.0.norm1.weight', 'first_stage_model.encoder.down.3.block.0.norm1.bias', 'first_stage_model.encoder.down.3.block.0.conv1.weight', 'first_stage_model.encoder.down.3.block.0.conv1.bias', 'first_stage_model.encoder.down.3.block.0.norm2.weight', 'first_stage_model.encoder.down.3.block.0.norm2.bias', 'first_stage_model.encoder.down.3.block.0.conv2.weight', 'first_stage_model.encoder.down.3.block.0.conv2.bias', 'first_stage_model.encoder.down.3.block.1.norm1.weight', 'first_stage_model.encoder.down.3.block.1.norm1.bias', 'first_stage_model.encoder.down.3.block.1.conv1.weight', 'first_stage_model.encoder.down.3.block.1.conv1.bias', 'first_stage_model.encoder.down.3.block.1.norm2.weight', 'first_stage_model.encoder.down.3.block.1.norm2.bias', 'first_stage_model.encoder.down.3.block.1.conv2.weight', 'first_stage_model.encoder.down.3.block.1.conv2.bias', 'first_stage_model.encoder.mid.block_1.norm1.weight', 'first_stage_model.encoder.mid.block_1.norm1.bias', 'first_stage_model.encoder.mid.block_1.conv1.weight', 'first_stage_model.encoder.mid.block_1.conv1.bias', 'first_stage_model.encoder.mid.block_1.norm2.weight', 'first_stage_model.encoder.mid.block_1.norm2.bias', 'first_stage_model.encoder.mid.block_1.conv2.weight', 'first_stage_model.encoder.mid.block_1.conv2.bias', 'first_stage_model.encoder.mid.attn_1.norm.weight', 'first_stage_model.encoder.mid.attn_1.norm.bias', 'first_stage_model.encoder.mid.attn_1.q.weight', 'first_stage_model.encoder.mid.attn_1.q.bias', 'first_stage_model.encoder.mid.attn_1.k.weight', 'first_stage_model.encoder.mid.attn_1.k.bias', 'first_stage_model.encoder.mid.attn_1.v.weight', 'first_stage_model.encoder.mid.attn_1.v.bias', 'first_stage_model.encoder.mid.attn_1.proj_out.weight', 'first_stage_model.encoder.mid.attn_1.proj_out.bias', 'first_stage_model.encoder.mid.block_2.norm1.weight', 'first_stage_model.encoder.mid.block_2.norm1.bias', 'first_stage_model.encoder.mid.block_2.conv1.weight', 'first_stage_model.encoder.mid.block_2.conv1.bias', 'first_stage_model.encoder.mid.block_2.norm2.weight', 'first_stage_model.encoder.mid.block_2.norm2.bias', 'first_stage_model.encoder.mid.block_2.conv2.weight', 'first_stage_model.encoder.mid.block_2.conv2.bias', 'first_stage_model.encoder.norm_out.weight', 'first_stage_model.encoder.norm_out.bias', 'first_stage_model.encoder.conv_out.weight', 'first_stage_model.encoder.conv_out.bias', 'first_stage_model.decoder.conv_in.weight', 'first_stage_model.decoder.conv_in.bias', 'first_stage_model.decoder.mid.block_1.norm1.weight', 'first_stage_model.decoder.mid.block_1.norm1.bias', 'first_stage_model.decoder.mid.block_1.conv1.weight', 'first_stage_model.decoder.mid.block_1.conv1.bias', 'first_stage_model.decoder.mid.block_1.norm2.weight', 'first_stage_model.decoder.mid.block_1.norm2.bias', 'first_stage_model.decoder.mid.block_1.conv2.weight', 'first_stage_model.decoder.mid.block_1.conv2.bias', 'first_stage_model.decoder.mid.attn_1.norm.weight', 'first_stage_model.decoder.mid.attn_1.norm.bias', 'first_stage_model.decoder.mid.attn_1.q.weight', 'first_stage_model.decoder.mid.attn_1.q.bias', 'first_stage_model.decoder.mid.attn_1.k.weight', 'first_stage_model.decoder.mid.attn_1.k.bias', 'first_stage_model.decoder.mid.attn_1.v.weight', 'first_stage_model.decoder.mid.attn_1.v.bias', 'first_stage_model.decoder.mid.attn_1.proj_out.weight', 'first_stage_model.decoder.mid.attn_1.proj_out.bias', 'first_stage_model.decoder.mid.block_2.norm1.weight', 'first_stage_model.decoder.mid.block_2.norm1.bias', 'first_stage_model.decoder.mid.block_2.conv1.weight', 'first_stage_model.decoder.mid.block_2.conv1.bias', 'first_stage_model.decoder.mid.block_2.norm2.weight', 'first_stage_model.decoder.mid.block_2.norm2.bias', 'first_stage_model.decoder.mid.block_2.conv2.weight', 'first_stage_model.decoder.mid.block_2.conv2.bias', 'first_stage_model.decoder.up.0.block.0.norm1.weight', 'first_stage_model.decoder.up.0.block.0.norm1.bias', 'first_stage_model.decoder.up.0.block.0.conv1.weight', 'first_stage_model.decoder.up.0.block.0.conv1.bias', 'first_stage_model.decoder.up.0.block.0.norm2.weight', 'first_stage_model.decoder.up.0.block.0.norm2.bias', 'first_stage_model.decoder.up.0.block.0.conv2.weight', 'first_stage_model.decoder.up.0.block.0.conv2.bias', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.0.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.0.block.1.norm1.weight', 'first_stage_model.decoder.up.0.block.1.norm1.bias', 'first_stage_model.decoder.up.0.block.1.conv1.weight', 'first_stage_model.decoder.up.0.block.1.conv1.bias', 'first_stage_model.decoder.up.0.block.1.norm2.weight', 'first_stage_model.decoder.up.0.block.1.norm2.bias', 'first_stage_model.decoder.up.0.block.1.conv2.weight', 'first_stage_model.decoder.up.0.block.1.conv2.bias', 'first_stage_model.decoder.up.0.block.2.norm1.weight', 'first_stage_model.decoder.up.0.block.2.norm1.bias', 'first_stage_model.decoder.up.0.block.2.conv1.weight', 'first_stage_model.decoder.up.0.block.2.conv1.bias', 'first_stage_model.decoder.up.0.block.2.norm2.weight', 'first_stage_model.decoder.up.0.block.2.norm2.bias', 'first_stage_model.decoder.up.0.block.2.conv2.weight', 'first_stage_model.decoder.up.0.block.2.conv2.bias', 'first_stage_model.decoder.up.1.block.0.norm1.weight', 'first_stage_model.decoder.up.1.block.0.norm1.bias', 'first_stage_model.decoder.up.1.block.0.conv1.weight', 'first_stage_model.decoder.up.1.block.0.conv1.bias', 'first_stage_model.decoder.up.1.block.0.norm2.weight', 'first_stage_model.decoder.up.1.block.0.norm2.bias', 'first_stage_model.decoder.up.1.block.0.conv2.weight', 'first_stage_model.decoder.up.1.block.0.conv2.bias', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.weight', 'first_stage_model.decoder.up.1.block.0.nin_shortcut.bias', 'first_stage_model.decoder.up.1.block.1.norm1.weight', 'first_stage_model.decoder.up.1.block.1.norm1.bias', 'first_stage_model.decoder.up.1.block.1.conv1.weight', 'first_stage_model.decoder.up.1.block.1.conv1.bias', 'first_stage_model.decoder.up.1.block.1.norm2.weight', 'first_stage_model.decoder.up.1.block.1.norm2.bias', 'first_stage_model.decoder.up.1.block.1.conv2.weight', 'first_stage_model.decoder.up.1.block.1.conv2.bias', 'first_stage_model.decoder.up.1.block.2.norm1.weight', 'first_stage_model.decoder.up.1.block.2.norm1.bias', 'first_stage_model.decoder.up.1.block.2.conv1.weight', 'first_stage_model.decoder.up.1.block.2.conv1.bias', 'first_stage_model.decoder.up.1.block.2.norm2.weight', 'first_stage_model.decoder.up.1.block.2.norm2.bias', 'first_stage_model.decoder.up.1.block.2.conv2.weight', 'first_stage_model.decoder.up.1.block.2.conv2.bias', 'first_stage_model.decoder.up.1.upsample.conv.weight', 'first_stage_model.decoder.up.1.upsample.conv.bias', 'first_stage_model.decoder.up.2.block.0.norm1.weight', 'first_stage_model.decoder.up.2.block.0.norm1.bias', 'first_stage_model.decoder.up.2.block.0.conv1.weight', 'first_stage_model.decoder.up.2.block.0.conv1.bias', 'first_stage_model.decoder.up.2.block.0.norm2.weight', 'first_stage_model.decoder.up.2.block.0.norm2.bias', 'first_stage_model.decoder.up.2.block.0.conv2.weight', 'first_stage_model.decoder.up.2.block.0.conv2.bias', 'first_stage_model.decoder.up.2.block.1.norm1.weight', 'first_stage_model.decoder.up.2.block.1.norm1.bias', 'first_stage_model.decoder.up.2.block.1.conv1.weight', 'first_stage_model.decoder.up.2.block.1.conv1.bias', 'first_stage_model.decoder.up.2.block.1.norm2.weight', 'first_stage_model.decoder.up.2.block.1.norm2.bias', 'first_stage_model.decoder.up.2.block.1.conv2.weight', 'first_stage_model.decoder.up.2.block.1.conv2.bias', 'first_stage_model.decoder.up.2.block.2.norm1.weight', 'first_stage_model.decoder.up.2.block.2.norm1.bias', 'first_stage_model.decoder.up.2.block.2.conv1.weight', 'first_stage_model.decoder.up.2.block.2.conv1.bias', 'first_stage_model.decoder.up.2.block.2.norm2.weight', 'first_stage_model.decoder.up.2.block.2.norm2.bias', 'first_stage_model.decoder.up.2.block.2.conv2.weight', 'first_stage_model.decoder.up.2.block.2.conv2.bias', 'first_stage_model.decoder.up.2.upsample.conv.weight', 'first_stage_model.decoder.up.2.upsample.conv.bias', 'first_stage_model.decoder.up.3.block.0.norm1.weight', 'first_stage_model.decoder.up.3.block.0.norm1.bias', 'first_stage_model.decoder.up.3.block.0.conv1.weight', 'first_stage_model.decoder.up.3.block.0.conv1.bias', 'first_stage_model.decoder.up.3.block.0.norm2.weight', 'first_stage_model.decoder.up.3.block.0.norm2.bias', 'first_stage_model.decoder.up.3.block.0.conv2.weight', 'first_stage_model.decoder.up.3.block.0.conv2.bias', 'first_stage_model.decoder.up.3.block.1.norm1.weight', 'first_stage_model.decoder.up.3.block.1.norm1.bias', 'first_stage_model.decoder.up.3.block.1.conv1.weight', 'first_stage_model.decoder.up.3.block.1.conv1.bias', 'first_stage_model.decoder.up.3.block.1.norm2.weight', 'first_stage_model.decoder.up.3.block.1.norm2.bias', 'first_stage_model.decoder.up.3.block.1.conv2.weight', 'first_stage_model.decoder.up.3.block.1.conv2.bias', 'first_stage_model.decoder.up.3.block.2.norm1.weight', 'first_stage_model.decoder.up.3.block.2.norm1.bias', 'first_stage_model.decoder.up.3.block.2.conv1.weight', 'first_stage_model.decoder.up.3.block.2.conv1.bias', 'first_stage_model.decoder.up.3.block.2.norm2.weight', 'first_stage_model.decoder.up.3.block.2.norm2.bias', 'first_stage_model.decoder.up.3.block.2.conv2.weight', 'first_stage_model.decoder.up.3.block.2.conv2.bias', 'first_stage_model.decoder.up.3.upsample.conv.weight', 'first_stage_model.decoder.up.3.upsample.conv.bias', 'first_stage_model.decoder.norm_out.weight', 'first_stage_model.decoder.norm_out.bias', 'first_stage_model.decoder.conv_out.weight', 'first_stage_model.decoder.conv_out.bias', 'first_stage_model.quant_conv.weight', 'first_stage_model.quant_conv.bias', 'first_stage_model.post_quant_conv.weight', 'first_stage_model.post_quant_conv.bias', 'cond_stage_model.model.positional_embedding', 'cond_stage_model.model.text_projection', 'cond_stage_model.model.logit_scale', 'cond_stage_model.model.transformer.resblocks.0.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.0.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.0.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.0.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.0.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.0.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.0.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.0.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.1.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.1.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.1.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.1.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.1.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.1.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.1.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.1.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.2.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.2.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.2.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.2.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.2.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.2.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.2.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.2.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.3.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.3.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.3.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.3.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.3.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.3.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.3.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.3.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.4.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.4.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.4.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.4.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.4.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.4.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.4.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.4.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.5.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.5.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.5.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.5.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.5.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.5.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.5.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.5.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.6.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.6.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.6.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.6.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.6.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.6.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.6.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.6.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.7.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.7.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.7.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.7.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.7.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.7.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.7.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.7.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.8.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.8.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.8.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.8.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.8.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.8.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.8.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.8.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.9.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.9.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.9.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.9.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.9.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.9.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.9.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.9.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.10.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.10.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.10.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.10.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.10.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.10.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.10.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.10.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.11.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.11.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.11.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.11.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.11.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.11.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.11.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.11.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.12.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.12.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.12.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.12.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.12.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.12.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.12.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.12.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.13.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.13.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.13.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.13.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.13.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.13.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.13.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.13.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.14.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.14.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.14.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.14.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.14.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.14.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.14.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.14.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.15.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.15.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.15.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.15.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.15.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.15.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.15.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.15.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.16.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.16.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.16.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.16.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.16.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.16.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.16.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.16.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.17.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.17.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.17.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.17.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.17.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.17.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.17.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.17.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.18.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.18.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.18.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.18.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.18.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.18.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.18.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.18.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.19.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.19.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.19.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.19.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.19.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.19.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.19.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.19.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.20.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.20.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.20.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.20.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.20.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.20.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.20.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.20.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.21.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.21.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.21.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.21.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.21.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.21.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.21.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.21.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.22.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.22.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.22.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.22.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.22.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.22.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.22.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.22.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.bias', 'cond_stage_model.model.transformer.resblocks.23.ln_1.weight', 'cond_stage_model.model.transformer.resblocks.23.ln_1.bias', 'cond_stage_model.model.transformer.resblocks.23.attn.in_proj_weight', 'cond_stage_model.model.transformer.resblocks.23.attn.in_proj_bias', 'cond_stage_model.model.transformer.resblocks.23.attn.out_proj.weight', 'cond_stage_model.model.transformer.resblocks.23.attn.out_proj.bias', 'cond_stage_model.model.transformer.resblocks.23.ln_2.weight', 'cond_stage_model.model.transformer.resblocks.23.ln_2.bias', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.weight', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.bias', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.weight', 'cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.bias', 'cond_stage_model.model.token_embedding.weight', 'cond_stage_model.model.ln_final.weight', 'cond_stage_model.model.ln_final.bias', 'embedder.model.positional_embedding', 'embedder.model.text_projection', 'embedder.model.logit_scale', 'embedder.model.visual.class_embedding', 'embedder.model.visual.positional_embedding', 'embedder.model.visual.proj', 'embedder.model.visual.conv1.weight', 'embedder.model.visual.ln_pre.weight', 'embedder.model.visual.ln_pre.bias', 'embedder.model.visual.transformer.resblocks.0.ln_1.weight', 'embedder.model.visual.transformer.resblocks.0.ln_1.bias', 'embedder.model.visual.transformer.resblocks.0.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.0.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.0.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.0.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.0.ln_2.weight', 'embedder.model.visual.transformer.resblocks.0.ln_2.bias', 'embedder.model.visual.transformer.resblocks.0.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.0.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.0.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.0.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.1.ln_1.weight', 'embedder.model.visual.transformer.resblocks.1.ln_1.bias', 'embedder.model.visual.transformer.resblocks.1.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.1.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.1.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.1.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.1.ln_2.weight', 'embedder.model.visual.transformer.resblocks.1.ln_2.bias', 'embedder.model.visual.transformer.resblocks.1.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.1.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.1.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.1.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.2.ln_1.weight', 'embedder.model.visual.transformer.resblocks.2.ln_1.bias', 'embedder.model.visual.transformer.resblocks.2.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.2.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.2.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.2.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.2.ln_2.weight', 'embedder.model.visual.transformer.resblocks.2.ln_2.bias', 'embedder.model.visual.transformer.resblocks.2.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.2.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.2.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.2.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.3.ln_1.weight', 'embedder.model.visual.transformer.resblocks.3.ln_1.bias', 'embedder.model.visual.transformer.resblocks.3.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.3.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.3.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.3.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.3.ln_2.weight', 'embedder.model.visual.transformer.resblocks.3.ln_2.bias', 'embedder.model.visual.transformer.resblocks.3.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.3.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.3.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.3.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.4.ln_1.weight', 'embedder.model.visual.transformer.resblocks.4.ln_1.bias', 'embedder.model.visual.transformer.resblocks.4.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.4.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.4.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.4.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.4.ln_2.weight', 'embedder.model.visual.transformer.resblocks.4.ln_2.bias', 'embedder.model.visual.transformer.resblocks.4.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.4.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.4.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.4.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.5.ln_1.weight', 'embedder.model.visual.transformer.resblocks.5.ln_1.bias', 'embedder.model.visual.transformer.resblocks.5.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.5.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.5.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.5.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.5.ln_2.weight', 'embedder.model.visual.transformer.resblocks.5.ln_2.bias', 'embedder.model.visual.transformer.resblocks.5.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.5.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.5.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.5.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.6.ln_1.weight', 'embedder.model.visual.transformer.resblocks.6.ln_1.bias', 'embedder.model.visual.transformer.resblocks.6.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.6.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.6.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.6.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.6.ln_2.weight', 'embedder.model.visual.transformer.resblocks.6.ln_2.bias', 'embedder.model.visual.transformer.resblocks.6.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.6.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.6.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.6.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.7.ln_1.weight', 'embedder.model.visual.transformer.resblocks.7.ln_1.bias', 'embedder.model.visual.transformer.resblocks.7.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.7.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.7.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.7.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.7.ln_2.weight', 'embedder.model.visual.transformer.resblocks.7.ln_2.bias', 'embedder.model.visual.transformer.resblocks.7.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.7.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.7.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.7.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.8.ln_1.weight', 'embedder.model.visual.transformer.resblocks.8.ln_1.bias', 'embedder.model.visual.transformer.resblocks.8.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.8.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.8.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.8.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.8.ln_2.weight', 'embedder.model.visual.transformer.resblocks.8.ln_2.bias', 'embedder.model.visual.transformer.resblocks.8.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.8.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.8.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.8.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.9.ln_1.weight', 'embedder.model.visual.transformer.resblocks.9.ln_1.bias', 'embedder.model.visual.transformer.resblocks.9.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.9.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.9.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.9.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.9.ln_2.weight', 'embedder.model.visual.transformer.resblocks.9.ln_2.bias', 'embedder.model.visual.transformer.resblocks.9.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.9.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.9.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.9.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.10.ln_1.weight', 'embedder.model.visual.transformer.resblocks.10.ln_1.bias', 'embedder.model.visual.transformer.resblocks.10.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.10.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.10.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.10.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.10.ln_2.weight', 'embedder.model.visual.transformer.resblocks.10.ln_2.bias', 'embedder.model.visual.transformer.resblocks.10.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.10.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.10.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.10.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.11.ln_1.weight', 'embedder.model.visual.transformer.resblocks.11.ln_1.bias', 'embedder.model.visual.transformer.resblocks.11.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.11.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.11.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.11.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.11.ln_2.weight', 'embedder.model.visual.transformer.resblocks.11.ln_2.bias', 'embedder.model.visual.transformer.resblocks.11.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.11.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.11.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.11.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.12.ln_1.weight', 'embedder.model.visual.transformer.resblocks.12.ln_1.bias', 'embedder.model.visual.transformer.resblocks.12.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.12.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.12.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.12.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.12.ln_2.weight', 'embedder.model.visual.transformer.resblocks.12.ln_2.bias', 'embedder.model.visual.transformer.resblocks.12.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.12.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.12.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.12.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.13.ln_1.weight', 'embedder.model.visual.transformer.resblocks.13.ln_1.bias', 'embedder.model.visual.transformer.resblocks.13.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.13.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.13.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.13.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.13.ln_2.weight', 'embedder.model.visual.transformer.resblocks.13.ln_2.bias', 'embedder.model.visual.transformer.resblocks.13.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.13.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.13.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.13.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.14.ln_1.weight', 'embedder.model.visual.transformer.resblocks.14.ln_1.bias', 'embedder.model.visual.transformer.resblocks.14.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.14.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.14.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.14.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.14.ln_2.weight', 'embedder.model.visual.transformer.resblocks.14.ln_2.bias', 'embedder.model.visual.transformer.resblocks.14.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.14.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.14.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.14.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.15.ln_1.weight', 'embedder.model.visual.transformer.resblocks.15.ln_1.bias', 'embedder.model.visual.transformer.resblocks.15.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.15.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.15.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.15.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.15.ln_2.weight', 'embedder.model.visual.transformer.resblocks.15.ln_2.bias', 'embedder.model.visual.transformer.resblocks.15.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.15.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.15.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.15.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.16.ln_1.weight', 'embedder.model.visual.transformer.resblocks.16.ln_1.bias', 'embedder.model.visual.transformer.resblocks.16.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.16.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.16.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.16.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.16.ln_2.weight', 'embedder.model.visual.transformer.resblocks.16.ln_2.bias', 'embedder.model.visual.transformer.resblocks.16.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.16.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.16.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.16.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.17.ln_1.weight', 'embedder.model.visual.transformer.resblocks.17.ln_1.bias', 'embedder.model.visual.transformer.resblocks.17.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.17.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.17.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.17.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.17.ln_2.weight', 'embedder.model.visual.transformer.resblocks.17.ln_2.bias', 'embedder.model.visual.transformer.resblocks.17.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.17.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.17.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.17.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.18.ln_1.weight', 'embedder.model.visual.transformer.resblocks.18.ln_1.bias', 'embedder.model.visual.transformer.resblocks.18.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.18.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.18.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.18.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.18.ln_2.weight', 'embedder.model.visual.transformer.resblocks.18.ln_2.bias', 'embedder.model.visual.transformer.resblocks.18.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.18.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.18.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.18.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.19.ln_1.weight', 'embedder.model.visual.transformer.resblocks.19.ln_1.bias', 'embedder.model.visual.transformer.resblocks.19.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.19.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.19.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.19.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.19.ln_2.weight', 'embedder.model.visual.transformer.resblocks.19.ln_2.bias', 'embedder.model.visual.transformer.resblocks.19.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.19.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.19.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.19.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.20.ln_1.weight', 'embedder.model.visual.transformer.resblocks.20.ln_1.bias', 'embedder.model.visual.transformer.resblocks.20.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.20.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.20.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.20.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.20.ln_2.weight', 'embedder.model.visual.transformer.resblocks.20.ln_2.bias', 'embedder.model.visual.transformer.resblocks.20.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.20.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.20.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.20.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.21.ln_1.weight', 'embedder.model.visual.transformer.resblocks.21.ln_1.bias', 'embedder.model.visual.transformer.resblocks.21.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.21.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.21.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.21.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.21.ln_2.weight', 'embedder.model.visual.transformer.resblocks.21.ln_2.bias', 'embedder.model.visual.transformer.resblocks.21.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.21.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.21.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.21.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.22.ln_1.weight', 'embedder.model.visual.transformer.resblocks.22.ln_1.bias', 'embedder.model.visual.transformer.resblocks.22.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.22.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.22.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.22.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.22.ln_2.weight', 'embedder.model.visual.transformer.resblocks.22.ln_2.bias', 'embedder.model.visual.transformer.resblocks.22.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.22.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.22.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.22.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.23.ln_1.weight', 'embedder.model.visual.transformer.resblocks.23.ln_1.bias', 'embedder.model.visual.transformer.resblocks.23.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.23.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.23.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.23.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.23.ln_2.weight', 'embedder.model.visual.transformer.resblocks.23.ln_2.bias', 'embedder.model.visual.transformer.resblocks.23.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.23.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.23.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.23.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.24.ln_1.weight', 'embedder.model.visual.transformer.resblocks.24.ln_1.bias', 'embedder.model.visual.transformer.resblocks.24.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.24.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.24.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.24.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.24.ln_2.weight', 'embedder.model.visual.transformer.resblocks.24.ln_2.bias', 'embedder.model.visual.transformer.resblocks.24.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.24.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.24.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.24.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.25.ln_1.weight', 'embedder.model.visual.transformer.resblocks.25.ln_1.bias', 'embedder.model.visual.transformer.resblocks.25.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.25.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.25.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.25.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.25.ln_2.weight', 'embedder.model.visual.transformer.resblocks.25.ln_2.bias', 'embedder.model.visual.transformer.resblocks.25.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.25.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.25.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.25.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.26.ln_1.weight', 'embedder.model.visual.transformer.resblocks.26.ln_1.bias', 'embedder.model.visual.transformer.resblocks.26.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.26.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.26.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.26.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.26.ln_2.weight', 'embedder.model.visual.transformer.resblocks.26.ln_2.bias', 'embedder.model.visual.transformer.resblocks.26.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.26.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.26.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.26.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.27.ln_1.weight', 'embedder.model.visual.transformer.resblocks.27.ln_1.bias', 'embedder.model.visual.transformer.resblocks.27.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.27.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.27.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.27.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.27.ln_2.weight', 'embedder.model.visual.transformer.resblocks.27.ln_2.bias', 'embedder.model.visual.transformer.resblocks.27.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.27.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.27.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.27.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.28.ln_1.weight', 'embedder.model.visual.transformer.resblocks.28.ln_1.bias', 'embedder.model.visual.transformer.resblocks.28.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.28.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.28.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.28.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.28.ln_2.weight', 'embedder.model.visual.transformer.resblocks.28.ln_2.bias', 'embedder.model.visual.transformer.resblocks.28.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.28.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.28.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.28.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.29.ln_1.weight', 'embedder.model.visual.transformer.resblocks.29.ln_1.bias', 'embedder.model.visual.transformer.resblocks.29.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.29.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.29.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.29.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.29.ln_2.weight', 'embedder.model.visual.transformer.resblocks.29.ln_2.bias', 'embedder.model.visual.transformer.resblocks.29.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.29.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.29.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.29.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.30.ln_1.weight', 'embedder.model.visual.transformer.resblocks.30.ln_1.bias', 'embedder.model.visual.transformer.resblocks.30.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.30.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.30.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.30.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.30.ln_2.weight', 'embedder.model.visual.transformer.resblocks.30.ln_2.bias', 'embedder.model.visual.transformer.resblocks.30.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.30.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.30.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.30.mlp.c_proj.bias', 'embedder.model.visual.transformer.resblocks.31.ln_1.weight', 'embedder.model.visual.transformer.resblocks.31.ln_1.bias', 'embedder.model.visual.transformer.resblocks.31.attn.in_proj_weight', 'embedder.model.visual.transformer.resblocks.31.attn.in_proj_bias', 'embedder.model.visual.transformer.resblocks.31.attn.out_proj.weight', 'embedder.model.visual.transformer.resblocks.31.attn.out_proj.bias', 'embedder.model.visual.transformer.resblocks.31.ln_2.weight', 'embedder.model.visual.transformer.resblocks.31.ln_2.bias', 'embedder.model.visual.transformer.resblocks.31.mlp.c_fc.weight', 'embedder.model.visual.transformer.resblocks.31.mlp.c_fc.bias', 'embedder.model.visual.transformer.resblocks.31.mlp.c_proj.weight', 'embedder.model.visual.transformer.resblocks.31.mlp.c_proj.bias', 'embedder.model.visual.ln_post.weight', 'embedder.model.visual.ln_post.bias', 'embedder.model.token_embedding.weight', 'embedder.model.ln_final.weight', 'embedder.model.ln_final.bias', 'image_proj_model.latents', 'image_proj_model.proj_in.weight', 'image_proj_model.proj_in.bias', 'image_proj_model.proj_out.weight', 'image_proj_model.proj_out.bias', 'image_proj_model.norm_out.weight', 'image_proj_model.norm_out.bias', 'image_proj_model.layers.0.0.norm1.weight', 'image_proj_model.layers.0.0.norm1.bias', 'image_proj_model.layers.0.0.norm2.weight', 'image_proj_model.layers.0.0.norm2.bias', 'image_proj_model.layers.0.0.to_q.weight', 'image_proj_model.layers.0.0.to_kv.weight', 'image_proj_model.layers.0.0.to_out.weight', 'image_proj_model.layers.0.1.0.weight', 'image_proj_model.layers.0.1.0.bias', 'image_proj_model.layers.0.1.1.weight', 'image_proj_model.layers.0.1.3.weight', 'image_proj_model.layers.1.0.norm1.weight', 'image_proj_model.layers.1.0.norm1.bias', 'image_proj_model.layers.1.0.norm2.weight', 'image_proj_model.layers.1.0.norm2.bias', 'image_proj_model.layers.1.0.to_q.weight', 'image_proj_model.layers.1.0.to_kv.weight', 'image_proj_model.layers.1.0.to_out.weight', 'image_proj_model.layers.1.1.0.weight', 'image_proj_model.layers.1.1.0.bias', 'image_proj_model.layers.1.1.1.weight', 'image_proj_model.layers.1.1.3.weight', 'image_proj_model.layers.2.0.norm1.weight', 'image_proj_model.layers.2.0.norm1.bias', 'image_proj_model.layers.2.0.norm2.weight', 'image_proj_model.layers.2.0.norm2.bias', 'image_proj_model.layers.2.0.to_q.weight', 'image_proj_model.layers.2.0.to_kv.weight', 'image_proj_model.layers.2.0.to_out.weight', 'image_proj_model.layers.2.1.0.weight', 'image_proj_model.layers.2.1.0.bias', 'image_proj_model.layers.2.1.1.weight', 'image_proj_model.layers.2.1.3.weight', 'image_proj_model.layers.3.0.norm1.weight', 'image_proj_model.layers.3.0.norm1.bias', 'image_proj_model.layers.3.0.norm2.weight', 'image_proj_model.layers.3.0.norm2.bias', 'image_proj_model.layers.3.0.to_q.weight', 'image_proj_model.layers.3.0.to_kv.weight', 'image_proj_model.layers.3.0.to_out.weight', 'image_proj_model.layers.3.1.0.weight', 'image_proj_model.layers.3.1.0.bias', 'image_proj_model.layers.3.1.1.weight', 'image_proj_model.layers.3.1.3.weight'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ad0565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[-9.5963e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.4771e-06]]],\n",
      "\n",
      "\n",
      "         [[[-8.8215e-06]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-9.5963e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.5367e-06]]],\n",
      "\n",
      "\n",
      "         [[[-2.4438e-06]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 8.6427e-06]]],\n",
      "\n",
      "\n",
      "         [[[-9.2983e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.7752e-06]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 7.8082e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.7156e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.8348e-06]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 8.5831e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 8.9407e-06]]],\n",
      "\n",
      "\n",
      "         [[[-9.7752e-06]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-9.1791e-06]]],\n",
      "\n",
      "\n",
      "         [[[-9.3579e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 7.4506e-06]]]],\n",
      "\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-8.4639e-06]]],\n",
      "\n",
      "\n",
      "         [[[-9.5963e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.6560e-06]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 9.2387e-06]]],\n",
      "\n",
      "\n",
      "         [[[-9.2983e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.4771e-06]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 9.2983e-06]]],\n",
      "\n",
      "\n",
      "         [[[-9.3579e-06]]],\n",
      "\n",
      "\n",
      "         [[[-8.6427e-06]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[ 9.6560e-06]]],\n",
      "\n",
      "\n",
      "         [[[-8.8811e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.4175e-06]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 8.7619e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 7.0930e-06]]],\n",
      "\n",
      "\n",
      "         [[[-9.4771e-06]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[-8.6427e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.2983e-06]]],\n",
      "\n",
      "\n",
      "         [[[ 9.5963e-06]]]]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print(ckpt[\"module\"][\"model.diffusion_model.context_encoder.zero_convolutions.0.conv.weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0ca65dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betas\n",
      "alphas_cumprod\n",
      "alphas_cumprod_prev\n",
      "sqrt_alphas_cumprod\n",
      "sqrt_one_minus_alphas_cumprod\n",
      "log_one_minus_alphas_cumprod\n",
      "sqrt_recip_alphas_cumprod\n",
      "sqrt_recipm1_alphas_cumprod\n",
      "posterior_variance\n",
      "posterior_log_variance_clipped\n",
      "posterior_mean_coef1\n",
      "posterior_mean_coef2\n",
      "model.diffusion_model.time_embed.0.weight\n",
      "model.diffusion_model.time_embed.0.bias\n",
      "model.diffusion_model.time_embed.2.weight\n",
      "model.diffusion_model.time_embed.2.bias\n",
      "model.diffusion_model.fps_embedding.0.weight\n",
      "model.diffusion_model.fps_embedding.0.bias\n",
      "model.diffusion_model.fps_embedding.2.weight\n",
      "model.diffusion_model.fps_embedding.2.bias\n",
      "model.diffusion_model.input_blocks.0.0.weight\n",
      "model.diffusion_model.input_blocks.0.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.1.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.input_blocks.1.1.norm.weight\n",
      "model.diffusion_model.input_blocks.1.1.norm.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.1.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.1.2.norm.weight\n",
      "model.diffusion_model.input_blocks.1.2.norm.bias\n",
      "model.diffusion_model.input_blocks.1.2.proj_in.weight\n",
      "model.diffusion_model.input_blocks.1.2.proj_in.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.1.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.1.2.proj_out.weight\n",
      "model.diffusion_model.input_blocks.1.2.proj_out.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.2.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.input_blocks.2.1.norm.weight\n",
      "model.diffusion_model.input_blocks.2.1.norm.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.2.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.2.2.norm.weight\n",
      "model.diffusion_model.input_blocks.2.2.norm.bias\n",
      "model.diffusion_model.input_blocks.2.2.proj_in.weight\n",
      "model.diffusion_model.input_blocks.2.2.proj_in.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.2.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.2.2.proj_out.weight\n",
      "model.diffusion_model.input_blocks.2.2.proj_out.bias\n",
      "model.diffusion_model.input_blocks.3.0.op.weight\n",
      "model.diffusion_model.input_blocks.3.0.op.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.4.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.input_blocks.4.1.norm.weight\n",
      "model.diffusion_model.input_blocks.4.1.norm.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.4.2.norm.weight\n",
      "model.diffusion_model.input_blocks.4.2.norm.bias\n",
      "model.diffusion_model.input_blocks.4.2.proj_in.weight\n",
      "model.diffusion_model.input_blocks.4.2.proj_in.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.4.2.proj_out.weight\n",
      "model.diffusion_model.input_blocks.4.2.proj_out.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.5.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.input_blocks.5.1.norm.weight\n",
      "model.diffusion_model.input_blocks.5.1.norm.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.5.2.norm.weight\n",
      "model.diffusion_model.input_blocks.5.2.norm.bias\n",
      "model.diffusion_model.input_blocks.5.2.proj_in.weight\n",
      "model.diffusion_model.input_blocks.5.2.proj_in.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.5.2.proj_out.weight\n",
      "model.diffusion_model.input_blocks.5.2.proj_out.bias\n",
      "model.diffusion_model.input_blocks.6.0.op.weight\n",
      "model.diffusion_model.input_blocks.6.0.op.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.input_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.7.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.input_blocks.7.1.norm.weight\n",
      "model.diffusion_model.input_blocks.7.1.norm.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.7.2.norm.weight\n",
      "model.diffusion_model.input_blocks.7.2.norm.bias\n",
      "model.diffusion_model.input_blocks.7.2.proj_in.weight\n",
      "model.diffusion_model.input_blocks.7.2.proj_in.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.7.2.proj_out.weight\n",
      "model.diffusion_model.input_blocks.7.2.proj_out.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.8.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.input_blocks.8.1.norm.weight\n",
      "model.diffusion_model.input_blocks.8.1.norm.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.input_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.input_blocks.8.2.norm.weight\n",
      "model.diffusion_model.input_blocks.8.2.norm.bias\n",
      "model.diffusion_model.input_blocks.8.2.proj_in.weight\n",
      "model.diffusion_model.input_blocks.8.2.proj_in.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.input_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.input_blocks.8.2.proj_out.weight\n",
      "model.diffusion_model.input_blocks.8.2.proj_out.bias\n",
      "model.diffusion_model.input_blocks.9.0.op.weight\n",
      "model.diffusion_model.input_blocks.9.0.op.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.10.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.input_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.input_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.input_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.input_blocks.11.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.init_attn.0.norm.weight\n",
      "model.diffusion_model.init_attn.0.norm.bias\n",
      "model.diffusion_model.init_attn.0.proj_in.weight\n",
      "model.diffusion_model.init_attn.0.proj_in.bias\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.init_attn.0.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.init_attn.0.proj_out.weight\n",
      "model.diffusion_model.init_attn.0.proj_out.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.0.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.0.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.0.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.0.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.middle_block.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.middle_block.1.norm.weight\n",
      "model.diffusion_model.middle_block.1.norm.bias\n",
      "model.diffusion_model.middle_block.1.proj_in.weight\n",
      "model.diffusion_model.middle_block.1.proj_in.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.middle_block.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.middle_block.1.proj_out.weight\n",
      "model.diffusion_model.middle_block.1.proj_out.bias\n",
      "model.diffusion_model.middle_block.2.norm.weight\n",
      "model.diffusion_model.middle_block.2.norm.bias\n",
      "model.diffusion_model.middle_block.2.proj_in.weight\n",
      "model.diffusion_model.middle_block.2.proj_in.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.middle_block.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.middle_block.2.proj_out.weight\n",
      "model.diffusion_model.middle_block.2.proj_out.bias\n",
      "model.diffusion_model.middle_block.3.in_layers.0.weight\n",
      "model.diffusion_model.middle_block.3.in_layers.0.bias\n",
      "model.diffusion_model.middle_block.3.in_layers.2.weight\n",
      "model.diffusion_model.middle_block.3.in_layers.2.bias\n",
      "model.diffusion_model.middle_block.3.emb_layers.1.weight\n",
      "model.diffusion_model.middle_block.3.emb_layers.1.bias\n",
      "model.diffusion_model.middle_block.3.out_layers.0.weight\n",
      "model.diffusion_model.middle_block.3.out_layers.0.bias\n",
      "model.diffusion_model.middle_block.3.out_layers.3.weight\n",
      "model.diffusion_model.middle_block.3.out_layers.3.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.middle_block.3.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.0.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.0.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.0.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.0.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.0.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.1.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.1.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.2.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.2.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.2.1.conv.weight\n",
      "model.diffusion_model.output_blocks.2.1.conv.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.3.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.3.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.3.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.3.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.3.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.3.1.norm.weight\n",
      "model.diffusion_model.output_blocks.3.1.norm.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.3.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.3.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.3.2.norm.weight\n",
      "model.diffusion_model.output_blocks.3.2.norm.bias\n",
      "model.diffusion_model.output_blocks.3.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.3.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.3.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.3.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.3.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.4.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.4.1.norm.weight\n",
      "model.diffusion_model.output_blocks.4.1.norm.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.4.2.norm.weight\n",
      "model.diffusion_model.output_blocks.4.2.norm.bias\n",
      "model.diffusion_model.output_blocks.4.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.4.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.4.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.4.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.4.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.5.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.5.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.5.1.norm.weight\n",
      "model.diffusion_model.output_blocks.5.1.norm.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.2.norm.weight\n",
      "model.diffusion_model.output_blocks.5.2.norm.bias\n",
      "model.diffusion_model.output_blocks.5.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.5.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.5.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.5.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.5.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.5.3.conv.weight\n",
      "model.diffusion_model.output_blocks.5.3.conv.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.6.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.6.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.6.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.6.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.6.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.6.1.norm.weight\n",
      "model.diffusion_model.output_blocks.6.1.norm.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.6.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.6.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.6.2.norm.weight\n",
      "model.diffusion_model.output_blocks.6.2.norm.bias\n",
      "model.diffusion_model.output_blocks.6.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.6.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.6.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.6.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.6.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.7.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.7.1.norm.weight\n",
      "model.diffusion_model.output_blocks.7.1.norm.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.7.2.norm.weight\n",
      "model.diffusion_model.output_blocks.7.2.norm.bias\n",
      "model.diffusion_model.output_blocks.7.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.7.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.7.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.7.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.7.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.8.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.8.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.8.1.norm.weight\n",
      "model.diffusion_model.output_blocks.8.1.norm.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.2.norm.weight\n",
      "model.diffusion_model.output_blocks.8.2.norm.bias\n",
      "model.diffusion_model.output_blocks.8.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.8.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.8.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.8.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.8.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.8.3.conv.weight\n",
      "model.diffusion_model.output_blocks.8.3.conv.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.9.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.9.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.9.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.9.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.9.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.9.1.norm.weight\n",
      "model.diffusion_model.output_blocks.9.1.norm.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.9.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.9.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.9.2.norm.weight\n",
      "model.diffusion_model.output_blocks.9.2.norm.bias\n",
      "model.diffusion_model.output_blocks.9.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.9.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.9.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.9.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.9.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.10.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.10.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.10.1.norm.weight\n",
      "model.diffusion_model.output_blocks.10.1.norm.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.10.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.10.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.10.2.norm.weight\n",
      "model.diffusion_model.output_blocks.10.2.norm.bias\n",
      "model.diffusion_model.output_blocks.10.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.10.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.10.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.10.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.10.2.proj_out.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.output_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.output_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.output_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.weight\n",
      "model.diffusion_model.output_blocks.11.0.skip_connection.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.output_blocks.11.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.output_blocks.11.1.norm.weight\n",
      "model.diffusion_model.output_blocks.11.1.norm.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_in.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.11.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.weight\n",
      "model.diffusion_model.output_blocks.11.1.proj_out.bias\n",
      "model.diffusion_model.output_blocks.11.2.norm.weight\n",
      "model.diffusion_model.output_blocks.11.2.norm.bias\n",
      "model.diffusion_model.output_blocks.11.2.proj_in.weight\n",
      "model.diffusion_model.output_blocks.11.2.proj_in.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.pluker_projection.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.pluker_projection.bias\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.epipolar.epipolar_attn.register_tokens\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.epipolar.epipolar_attn.to_q.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.epipolar.epipolar_attn.to_k.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.epipolar.epipolar_attn.to_v.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.weight\n",
      "model.diffusion_model.output_blocks.11.2.transformer_blocks.0.epipolar.epipolar_attn.to_out.0.bias\n",
      "model.diffusion_model.output_blocks.11.2.proj_out.weight\n",
      "model.diffusion_model.output_blocks.11.2.proj_out.bias\n",
      "model.diffusion_model.out.0.weight\n",
      "model.diffusion_model.out.0.bias\n",
      "model.diffusion_model.out.2.weight\n",
      "model.diffusion_model.out.2.bias\n",
      "model.diffusion_model.context_encoder.time_embed.0.weight\n",
      "model.diffusion_model.context_encoder.time_embed.0.bias\n",
      "model.diffusion_model.context_encoder.time_embed.2.weight\n",
      "model.diffusion_model.context_encoder.time_embed.2.bias\n",
      "model.diffusion_model.context_encoder.fps_embedding.0.weight\n",
      "model.diffusion_model.context_encoder.fps_embedding.0.bias\n",
      "model.diffusion_model.context_encoder.fps_embedding.2.weight\n",
      "model.diffusion_model.context_encoder.fps_embedding.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.0.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.0.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.1.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.1.2.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.1.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.2.2.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.3.0.op.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.3.0.op.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.skip_connection.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.skip_connection.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.1.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.4.2.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.1.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.5.2.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.6.0.op.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.6.0.op.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.skip_connection.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.skip_connection.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.1.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.7.2.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.alpha\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.to_k_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.attn2.to_v_ip.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.1.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.norm.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.norm.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.proj_in.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.proj_in.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.proj_out.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.8.2.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.9.0.op.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.9.0.op.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.10.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.in_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.in_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.in_layers.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.in_layers.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.emb_layers.1.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.emb_layers.1.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.out_layers.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.out_layers.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.out_layers.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.out_layers.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv1.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv1.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv1.2.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv1.2.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv2.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv2.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv2.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv2.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv3.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv3.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv3.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv3.3.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv4.0.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv4.0.bias\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv4.3.weight\n",
      "model.diffusion_model.context_encoder.input_blocks.11.0.temopral_conv.conv4.3.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.norm.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.norm.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.proj_in.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.proj_in.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn1.to_q.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn1.to_k.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn1.to_v.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn1.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn1.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.ff.net.0.proj.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.ff.net.0.proj.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.ff.net.2.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.ff.net.2.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn2.to_q.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn2.to_k.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn2.to_v.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn2.to_out.0.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.attn2.to_out.0.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.norm1.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.norm1.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.norm2.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.norm2.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.norm3.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.transformer_blocks.0.norm3.bias\n",
      "model.diffusion_model.context_encoder.init_attn.0.proj_out.weight\n",
      "model.diffusion_model.context_encoder.init_attn.0.proj_out.bias\n",
      "model.diffusion_model.context_encoder.input_zero_convolution.conv.weight\n",
      "model.diffusion_model.context_encoder.input_zero_convolution.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.0.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.0.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.1.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.1.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.2.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.2.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.3.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.3.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.4.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.4.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.5.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.5.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.6.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.6.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.7.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.7.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.8.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.8.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.9.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.9.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.10.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.10.conv.bias\n",
      "model.diffusion_model.context_encoder.zero_convolutions.11.conv.weight\n",
      "model.diffusion_model.context_encoder.zero_convolutions.11.conv.bias\n",
      "first_stage_model.encoder.conv_in.weight\n",
      "first_stage_model.encoder.conv_in.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.0.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.0.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.0.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.0.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.0.downsample.conv.weight\n",
      "first_stage_model.encoder.down.0.downsample.conv.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.1.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.1.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.1.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.1.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.1.downsample.conv.weight\n",
      "first_stage_model.encoder.down.1.downsample.conv.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.weight\n",
      "first_stage_model.encoder.down.2.block.0.nin_shortcut.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.2.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.2.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.2.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.2.block.1.conv2.bias\n",
      "first_stage_model.encoder.down.2.downsample.conv.weight\n",
      "first_stage_model.encoder.down.2.downsample.conv.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.0.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.0.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.0.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.0.conv2.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm1.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm1.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv1.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv1.bias\n",
      "first_stage_model.encoder.down.3.block.1.norm2.weight\n",
      "first_stage_model.encoder.down.3.block.1.norm2.bias\n",
      "first_stage_model.encoder.down.3.block.1.conv2.weight\n",
      "first_stage_model.encoder.down.3.block.1.conv2.bias\n",
      "first_stage_model.encoder.mid.block_1.norm1.weight\n",
      "first_stage_model.encoder.mid.block_1.norm1.bias\n",
      "first_stage_model.encoder.mid.block_1.conv1.weight\n",
      "first_stage_model.encoder.mid.block_1.conv1.bias\n",
      "first_stage_model.encoder.mid.block_1.norm2.weight\n",
      "first_stage_model.encoder.mid.block_1.norm2.bias\n",
      "first_stage_model.encoder.mid.block_1.conv2.weight\n",
      "first_stage_model.encoder.mid.block_1.conv2.bias\n",
      "first_stage_model.encoder.mid.attn_1.norm.weight\n",
      "first_stage_model.encoder.mid.attn_1.norm.bias\n",
      "first_stage_model.encoder.mid.attn_1.q.weight\n",
      "first_stage_model.encoder.mid.attn_1.q.bias\n",
      "first_stage_model.encoder.mid.attn_1.k.weight\n",
      "first_stage_model.encoder.mid.attn_1.k.bias\n",
      "first_stage_model.encoder.mid.attn_1.v.weight\n",
      "first_stage_model.encoder.mid.attn_1.v.bias\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.encoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.encoder.mid.block_2.norm1.weight\n",
      "first_stage_model.encoder.mid.block_2.norm1.bias\n",
      "first_stage_model.encoder.mid.block_2.conv1.weight\n",
      "first_stage_model.encoder.mid.block_2.conv1.bias\n",
      "first_stage_model.encoder.mid.block_2.norm2.weight\n",
      "first_stage_model.encoder.mid.block_2.norm2.bias\n",
      "first_stage_model.encoder.mid.block_2.conv2.weight\n",
      "first_stage_model.encoder.mid.block_2.conv2.bias\n",
      "first_stage_model.encoder.norm_out.weight\n",
      "first_stage_model.encoder.norm_out.bias\n",
      "first_stage_model.encoder.conv_out.weight\n",
      "first_stage_model.encoder.conv_out.bias\n",
      "first_stage_model.decoder.conv_in.weight\n",
      "first_stage_model.decoder.conv_in.bias\n",
      "first_stage_model.decoder.mid.block_1.norm1.weight\n",
      "first_stage_model.decoder.mid.block_1.norm1.bias\n",
      "first_stage_model.decoder.mid.block_1.conv1.weight\n",
      "first_stage_model.decoder.mid.block_1.conv1.bias\n",
      "first_stage_model.decoder.mid.block_1.norm2.weight\n",
      "first_stage_model.decoder.mid.block_1.norm2.bias\n",
      "first_stage_model.decoder.mid.block_1.conv2.weight\n",
      "first_stage_model.decoder.mid.block_1.conv2.bias\n",
      "first_stage_model.decoder.mid.attn_1.norm.weight\n",
      "first_stage_model.decoder.mid.attn_1.norm.bias\n",
      "first_stage_model.decoder.mid.attn_1.q.weight\n",
      "first_stage_model.decoder.mid.attn_1.q.bias\n",
      "first_stage_model.decoder.mid.attn_1.k.weight\n",
      "first_stage_model.decoder.mid.attn_1.k.bias\n",
      "first_stage_model.decoder.mid.attn_1.v.weight\n",
      "first_stage_model.decoder.mid.attn_1.v.bias\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.weight\n",
      "first_stage_model.decoder.mid.attn_1.proj_out.bias\n",
      "first_stage_model.decoder.mid.block_2.norm1.weight\n",
      "first_stage_model.decoder.mid.block_2.norm1.bias\n",
      "first_stage_model.decoder.mid.block_2.conv1.weight\n",
      "first_stage_model.decoder.mid.block_2.conv1.bias\n",
      "first_stage_model.decoder.mid.block_2.norm2.weight\n",
      "first_stage_model.decoder.mid.block_2.norm2.bias\n",
      "first_stage_model.decoder.mid.block_2.conv2.weight\n",
      "first_stage_model.decoder.mid.block_2.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.0.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.0.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.0.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.0.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.0.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.weight\n",
      "first_stage_model.decoder.up.1.block.0.nin_shortcut.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.1.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.1.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.1.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.1.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.1.upsample.conv.weight\n",
      "first_stage_model.decoder.up.1.upsample.conv.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.2.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.2.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.2.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.2.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.2.upsample.conv.weight\n",
      "first_stage_model.decoder.up.2.upsample.conv.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.0.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.0.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.0.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.0.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.1.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.1.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.1.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.1.conv2.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm1.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm1.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv1.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv1.bias\n",
      "first_stage_model.decoder.up.3.block.2.norm2.weight\n",
      "first_stage_model.decoder.up.3.block.2.norm2.bias\n",
      "first_stage_model.decoder.up.3.block.2.conv2.weight\n",
      "first_stage_model.decoder.up.3.block.2.conv2.bias\n",
      "first_stage_model.decoder.up.3.upsample.conv.weight\n",
      "first_stage_model.decoder.up.3.upsample.conv.bias\n",
      "first_stage_model.decoder.norm_out.weight\n",
      "first_stage_model.decoder.norm_out.bias\n",
      "first_stage_model.decoder.conv_out.weight\n",
      "first_stage_model.decoder.conv_out.bias\n",
      "first_stage_model.quant_conv.weight\n",
      "first_stage_model.quant_conv.bias\n",
      "first_stage_model.post_quant_conv.weight\n",
      "first_stage_model.post_quant_conv.bias\n",
      "cond_stage_model.model.positional_embedding\n",
      "cond_stage_model.model.text_projection\n",
      "cond_stage_model.model.logit_scale\n",
      "cond_stage_model.model.transformer.resblocks.0.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.0.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.0.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.0.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.0.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.0.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.0.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.0.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.0.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.0.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.1.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.1.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.1.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.1.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.1.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.1.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.1.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.1.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.1.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.1.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.2.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.2.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.2.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.2.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.2.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.2.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.2.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.2.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.2.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.2.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.3.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.3.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.3.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.3.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.3.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.3.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.3.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.3.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.3.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.3.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.4.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.4.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.4.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.4.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.4.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.4.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.4.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.4.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.4.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.4.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.5.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.5.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.5.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.5.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.5.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.5.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.5.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.5.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.5.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.5.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.6.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.6.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.6.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.6.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.6.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.6.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.6.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.6.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.6.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.6.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.7.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.7.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.7.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.7.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.7.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.7.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.7.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.7.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.7.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.7.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.8.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.8.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.8.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.8.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.8.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.8.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.8.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.8.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.8.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.8.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.9.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.9.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.9.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.9.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.9.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.9.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.9.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.9.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.9.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.9.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.10.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.10.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.10.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.10.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.10.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.10.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.10.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.10.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.10.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.10.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.11.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.11.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.11.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.11.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.11.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.11.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.11.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.11.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.11.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.11.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.12.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.12.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.12.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.12.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.12.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.12.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.12.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.12.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.12.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.12.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.13.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.13.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.13.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.13.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.13.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.13.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.13.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.13.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.13.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.13.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.14.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.14.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.14.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.14.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.14.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.14.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.14.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.14.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.14.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.14.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.15.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.15.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.15.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.15.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.15.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.15.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.15.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.15.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.15.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.15.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.16.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.16.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.16.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.16.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.16.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.16.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.16.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.16.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.16.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.16.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.17.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.17.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.17.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.17.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.17.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.17.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.17.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.17.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.17.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.17.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.18.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.18.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.18.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.18.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.18.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.18.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.18.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.18.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.18.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.18.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.19.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.19.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.19.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.19.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.19.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.19.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.19.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.19.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.19.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.19.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.20.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.20.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.20.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.20.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.20.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.20.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.20.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.20.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.20.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.20.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.21.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.21.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.21.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.21.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.21.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.21.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.21.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.21.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.21.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.21.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.22.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.22.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.22.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.22.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.22.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.22.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.22.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.22.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.22.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.22.mlp.c_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.23.ln_1.weight\n",
      "cond_stage_model.model.transformer.resblocks.23.ln_1.bias\n",
      "cond_stage_model.model.transformer.resblocks.23.attn.in_proj_weight\n",
      "cond_stage_model.model.transformer.resblocks.23.attn.in_proj_bias\n",
      "cond_stage_model.model.transformer.resblocks.23.attn.out_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.23.attn.out_proj.bias\n",
      "cond_stage_model.model.transformer.resblocks.23.ln_2.weight\n",
      "cond_stage_model.model.transformer.resblocks.23.ln_2.bias\n",
      "cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.weight\n",
      "cond_stage_model.model.transformer.resblocks.23.mlp.c_fc.bias\n",
      "cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.weight\n",
      "cond_stage_model.model.transformer.resblocks.23.mlp.c_proj.bias\n",
      "cond_stage_model.model.token_embedding.weight\n",
      "cond_stage_model.model.ln_final.weight\n",
      "cond_stage_model.model.ln_final.bias\n",
      "embedder.model.positional_embedding\n",
      "embedder.model.text_projection\n",
      "embedder.model.logit_scale\n",
      "embedder.model.visual.class_embedding\n",
      "embedder.model.visual.positional_embedding\n",
      "embedder.model.visual.proj\n",
      "embedder.model.visual.conv1.weight\n",
      "embedder.model.visual.ln_pre.weight\n",
      "embedder.model.visual.ln_pre.bias\n",
      "embedder.model.visual.transformer.resblocks.0.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.0.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.0.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.0.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.0.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.0.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.0.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.0.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.0.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.0.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.0.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.0.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.1.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.1.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.1.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.1.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.1.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.1.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.1.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.1.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.1.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.1.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.1.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.1.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.2.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.2.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.2.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.2.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.2.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.2.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.2.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.2.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.2.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.2.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.2.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.2.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.3.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.3.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.3.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.3.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.3.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.3.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.3.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.3.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.3.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.3.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.3.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.3.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.4.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.4.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.4.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.4.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.4.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.4.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.4.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.4.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.4.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.4.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.4.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.4.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.5.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.5.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.5.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.5.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.5.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.5.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.5.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.5.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.5.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.5.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.5.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.5.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.6.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.6.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.6.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.6.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.6.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.6.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.6.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.6.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.6.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.6.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.6.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.6.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.7.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.7.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.7.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.7.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.7.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.7.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.7.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.7.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.7.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.7.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.7.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.7.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.8.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.8.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.8.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.8.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.8.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.8.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.8.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.8.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.8.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.8.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.8.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.8.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.9.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.9.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.9.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.9.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.9.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.9.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.9.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.9.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.9.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.9.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.9.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.9.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.10.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.10.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.10.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.10.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.10.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.10.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.10.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.10.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.10.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.10.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.10.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.10.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.11.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.11.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.11.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.11.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.11.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.11.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.11.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.11.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.11.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.11.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.11.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.11.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.12.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.12.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.12.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.12.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.12.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.12.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.12.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.12.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.12.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.12.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.12.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.12.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.13.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.13.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.13.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.13.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.13.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.13.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.13.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.13.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.13.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.13.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.13.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.13.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.14.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.14.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.14.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.14.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.14.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.14.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.14.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.14.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.14.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.14.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.14.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.14.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.15.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.15.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.15.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.15.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.15.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.15.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.15.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.15.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.15.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.15.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.15.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.15.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.16.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.16.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.16.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.16.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.16.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.16.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.16.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.16.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.16.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.16.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.16.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.16.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.17.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.17.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.17.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.17.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.17.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.17.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.17.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.17.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.17.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.17.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.17.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.17.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.18.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.18.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.18.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.18.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.18.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.18.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.18.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.18.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.18.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.18.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.18.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.18.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.19.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.19.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.19.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.19.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.19.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.19.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.19.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.19.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.19.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.19.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.19.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.19.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.20.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.20.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.20.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.20.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.20.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.20.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.20.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.20.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.20.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.20.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.20.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.20.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.21.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.21.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.21.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.21.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.21.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.21.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.21.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.21.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.21.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.21.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.21.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.21.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.22.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.22.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.22.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.22.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.22.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.22.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.22.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.22.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.22.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.22.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.22.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.22.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.23.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.23.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.23.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.23.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.23.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.23.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.23.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.23.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.23.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.23.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.23.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.23.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.24.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.24.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.24.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.24.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.24.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.24.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.24.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.24.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.24.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.24.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.24.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.24.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.25.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.25.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.25.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.25.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.25.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.25.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.25.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.25.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.25.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.25.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.25.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.25.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.26.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.26.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.26.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.26.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.26.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.26.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.26.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.26.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.26.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.26.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.26.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.26.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.27.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.27.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.27.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.27.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.27.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.27.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.27.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.27.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.27.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.27.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.27.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.27.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.28.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.28.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.28.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.28.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.28.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.28.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.28.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.28.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.28.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.28.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.28.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.28.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.29.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.29.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.29.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.29.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.29.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.29.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.29.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.29.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.29.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.29.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.29.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.29.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.30.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.30.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.30.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.30.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.30.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.30.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.30.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.30.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.30.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.30.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.30.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.30.mlp.c_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.31.ln_1.weight\n",
      "embedder.model.visual.transformer.resblocks.31.ln_1.bias\n",
      "embedder.model.visual.transformer.resblocks.31.attn.in_proj_weight\n",
      "embedder.model.visual.transformer.resblocks.31.attn.in_proj_bias\n",
      "embedder.model.visual.transformer.resblocks.31.attn.out_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.31.attn.out_proj.bias\n",
      "embedder.model.visual.transformer.resblocks.31.ln_2.weight\n",
      "embedder.model.visual.transformer.resblocks.31.ln_2.bias\n",
      "embedder.model.visual.transformer.resblocks.31.mlp.c_fc.weight\n",
      "embedder.model.visual.transformer.resblocks.31.mlp.c_fc.bias\n",
      "embedder.model.visual.transformer.resblocks.31.mlp.c_proj.weight\n",
      "embedder.model.visual.transformer.resblocks.31.mlp.c_proj.bias\n",
      "embedder.model.visual.ln_post.weight\n",
      "embedder.model.visual.ln_post.bias\n",
      "embedder.model.token_embedding.weight\n",
      "embedder.model.ln_final.weight\n",
      "embedder.model.ln_final.bias\n",
      "image_proj_model.latents\n",
      "image_proj_model.proj_in.weight\n",
      "image_proj_model.proj_in.bias\n",
      "image_proj_model.proj_out.weight\n",
      "image_proj_model.proj_out.bias\n",
      "image_proj_model.norm_out.weight\n",
      "image_proj_model.norm_out.bias\n",
      "image_proj_model.layers.0.0.norm1.weight\n",
      "image_proj_model.layers.0.0.norm1.bias\n",
      "image_proj_model.layers.0.0.norm2.weight\n",
      "image_proj_model.layers.0.0.norm2.bias\n",
      "image_proj_model.layers.0.0.to_q.weight\n",
      "image_proj_model.layers.0.0.to_kv.weight\n",
      "image_proj_model.layers.0.0.to_out.weight\n",
      "image_proj_model.layers.0.1.0.weight\n",
      "image_proj_model.layers.0.1.0.bias\n",
      "image_proj_model.layers.0.1.1.weight\n",
      "image_proj_model.layers.0.1.3.weight\n",
      "image_proj_model.layers.1.0.norm1.weight\n",
      "image_proj_model.layers.1.0.norm1.bias\n",
      "image_proj_model.layers.1.0.norm2.weight\n",
      "image_proj_model.layers.1.0.norm2.bias\n",
      "image_proj_model.layers.1.0.to_q.weight\n",
      "image_proj_model.layers.1.0.to_kv.weight\n",
      "image_proj_model.layers.1.0.to_out.weight\n",
      "image_proj_model.layers.1.1.0.weight\n",
      "image_proj_model.layers.1.1.0.bias\n",
      "image_proj_model.layers.1.1.1.weight\n",
      "image_proj_model.layers.1.1.3.weight\n",
      "image_proj_model.layers.2.0.norm1.weight\n",
      "image_proj_model.layers.2.0.norm1.bias\n",
      "image_proj_model.layers.2.0.norm2.weight\n",
      "image_proj_model.layers.2.0.norm2.bias\n",
      "image_proj_model.layers.2.0.to_q.weight\n",
      "image_proj_model.layers.2.0.to_kv.weight\n",
      "image_proj_model.layers.2.0.to_out.weight\n",
      "image_proj_model.layers.2.1.0.weight\n",
      "image_proj_model.layers.2.1.0.bias\n",
      "image_proj_model.layers.2.1.1.weight\n",
      "image_proj_model.layers.2.1.3.weight\n",
      "image_proj_model.layers.3.0.norm1.weight\n",
      "image_proj_model.layers.3.0.norm1.bias\n",
      "image_proj_model.layers.3.0.norm2.weight\n",
      "image_proj_model.layers.3.0.norm2.bias\n",
      "image_proj_model.layers.3.0.to_q.weight\n",
      "image_proj_model.layers.3.0.to_kv.weight\n",
      "image_proj_model.layers.3.0.to_out.weight\n",
      "image_proj_model.layers.3.1.0.weight\n",
      "image_proj_model.layers.3.1.0.bias\n",
      "image_proj_model.layers.3.1.1.weight\n",
      "image_proj_model.layers.3.1.3.weight\n",
      "image_proj_model.timestep_embedding_func.0.weight\n",
      "image_proj_model.timestep_embedding_func.0.bias\n",
      "image_proj_model.timestep_embedding_func.2.weight\n",
      "image_proj_model.timestep_embedding_func.2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.0.0.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.0.0.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.0.0.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.0.0.block2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.0.1.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.0.1.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.0.1.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.0.1.block2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.1.0.in_conv.weight\n",
      "pose_encoder.encoder_down_conv_blocks.1.0.in_conv.bias\n",
      "pose_encoder.encoder_down_conv_blocks.1.0.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.1.0.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.1.0.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.1.0.block2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.1.1.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.1.1.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.1.1.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.1.1.block2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.2.0.in_conv.weight\n",
      "pose_encoder.encoder_down_conv_blocks.2.0.in_conv.bias\n",
      "pose_encoder.encoder_down_conv_blocks.2.0.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.2.0.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.2.0.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.2.0.block2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.2.1.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.2.1.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.2.1.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.2.1.block2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.3.0.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.3.0.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.3.0.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.3.0.block2.bias\n",
      "pose_encoder.encoder_down_conv_blocks.3.1.block1.weight\n",
      "pose_encoder.encoder_down_conv_blocks.3.1.block1.bias\n",
      "pose_encoder.encoder_down_conv_blocks.3.1.block2.weight\n",
      "pose_encoder.encoder_down_conv_blocks.3.1.block2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.0.ff_norm.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.0.1.ff_norm.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.0.ff_norm.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.1.1.ff_norm.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.0.ff_norm.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.2.1.ff_norm.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.0.ff_norm.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.attention_blocks.0.to_q.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.attention_blocks.0.to_k.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.attention_blocks.0.to_v.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.attention_blocks.0.to_out.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.attention_blocks.0.to_out.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.attention_blocks.0.pos_encoder.pe\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.norms.0.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.norms.0.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.ff.net.0.proj.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.ff.net.0.proj.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.ff.net.2.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.ff.net.2.bias\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.ff_norm.weight\n",
      "pose_encoder.encoder_down_attention_blocks.3.1.ff_norm.bias\n",
      "pose_encoder.encoder_conv_in.weight\n",
      "pose_encoder.encoder_conv_in.bias\n"
     ]
    }
   ],
   "source": [
    "for k in ckpt[\"module\"].keys():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59003e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['metadata', 'state_dict', 'totals', 'options'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e37700dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_names = list(checkpoint_info['state_dict'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9803e0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== input_blocks.0 ===\n",
      "Total params: 26\n",
      "Submodules: ['emb_layers', 'in_layers', 'out_layers', 'temopral_conv']\n",
      "\n",
      "=== input_blocks.1 ===\n",
      "Total params: 29\n",
      "Submodules: ['norm', 'proj_in', 'proj_out', 'transformer_blocks']\n",
      "\n",
      "=== input_blocks.2 ===\n",
      "Total params: 34\n",
      "Submodules: ['norm', 'proj_in', 'proj_out', 'transformer_blocks']\n",
      "\n",
      "=== input_blocks.3 ===\n",
      "Total params: 26\n",
      "Submodules: ['emb_layers', 'in_layers', 'out_layers', 'temopral_conv']\n",
      "\n",
      "=== input_blocks.4 ===\n",
      "Total params: 0\n",
      "Submodules: []\n",
      "\n",
      "=== input_blocks.5 ===\n",
      "Total params: 0\n",
      "Submodules: []\n",
      "\n",
      "=== input_blocks.6 ===\n",
      "Total params: 0\n",
      "Submodules: []\n",
      "\n",
      "=== input_blocks.7 ===\n",
      "Total params: 0\n",
      "Submodules: []\n",
      "\n",
      "=== input_blocks.8 ===\n",
      "Total params: 0\n",
      "Submodules: []\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "\n",
    "def extract_input_block_layers(keys, start_idx=0, end_idx=8, root=\"model.diffusion_model.middle_block\"):\n",
    "    \"\"\"\n",
    "    Extract parameters that live under model.diffusion_model.input_blocks.{i}\n",
    "    for i in [start_idx, end_idx] inclusive.\n",
    "\n",
    "    Returns a nested dict:\n",
    "    {\n",
    "      i: {\n",
    "        \"all\": [full_param_key, ...],                      # all raw keys for this block\n",
    "        \"by_submodule\": { submodule: [keys...], ... },     # grouped by the token right after input_blocks.i\n",
    "        \"unique_submodules\": {submodule, ...}              # convenience set\n",
    "      },\n",
    "      ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    # Precompile regex: start of string, root, dot, (i), dot, capture next token (no dot), then the rest\n",
    "    # Example match groups for \"model.diffusion_model.input_blocks.2.0.emb_layers.1.bias\":\n",
    "    #   block_i = \"2\", submodule = \"0\", remainder = \"emb_layers.1.bias\"\n",
    "    pattern = re.compile(rf\"^{re.escape(root)}\\.(\\d+)\\.([^\\.]+)(?:\\.(.*))?$\")\n",
    "\n",
    "    # Prepare containers for each i\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        results[i] = {\n",
    "            \"all\": [],\n",
    "            \"by_submodule\": defaultdict(list),\n",
    "            \"unique_submodules\": set(),\n",
    "        }\n",
    "\n",
    "    for k in keys:\n",
    "        m = pattern.match(k)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        block_i_str, submodule, remainder = m.groups()\n",
    "        try:\n",
    "            block_i = int(block_i_str)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # Keep only requested range\n",
    "        if block_i < start_idx or block_i > end_idx:\n",
    "            continue\n",
    "\n",
    "        # Store\n",
    "        results[block_i][\"all\"].append(k)\n",
    "        results[block_i][\"by_submodule\"][submodule].append(k)\n",
    "        results[block_i][\"unique_submodules\"].add(submodule)\n",
    "\n",
    "    # Turn defaultdicts into plain dicts for cleanliness\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        results[i][\"by_submodule\"] = dict(results[i][\"by_submodule\"])\n",
    "\n",
    "    return results\n",
    "\n",
    "# --------- Example usage ----------\n",
    "blocks = extract_input_block_layers(parameter_names, start_idx=0, end_idx=8)\n",
    "\n",
    "# Print a concise summary:\n",
    "for i in range(0, 9):\n",
    "    print(f\"\\n=== input_blocks.{i} ===\")\n",
    "    print(f\"Total params: {len(blocks[i]['all'])}\")\n",
    "    with open(f\"../model_architecture/middle_block_{i}.txt\", \"w\") as f:\n",
    "        for k in blocks[i][\"all\"]:\n",
    "            f.write(k + \"\\n\")\n",
    "    print(\"Submodules:\", sorted(blocks[i][\"unique_submodules\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cami2v",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
